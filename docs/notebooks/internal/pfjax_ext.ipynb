{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f21d721",
   "metadata": {},
   "source": [
    "# Introduction to PFJAX\n",
    "\n",
    "**Martin Lysy -- University of Waterloo**\n",
    "\n",
    "**September 1, 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c124e87",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The goal of **PFJAX** is to provide tools for estimating the parameters $\\tth$ of a [state-space model](http://www.scholarpedia.org/article/State_space_model)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\xx_0 & \\sim \\pi(\\xx_0 \\mid \\tth) \\\\\n",
    "\\xx_t & \\sim f(\\xx_t \\mid \\xx_{t-1}, \\tth) \\\\\n",
    "\\yy_t & \\sim g(\\yy_t \\mid \\xx_t, \\tth).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In such models, only the *measurement variables* $\\yy_{0:T} = (\\yy_0, \\ldots, \\yy_T)$ are observed, whereas the *state variables* $\\xx_{0:T}$ are latent.  The marginal likelihood given the observed data is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Ell(\\tth) & = p(\\yy_{0:T} \\mid \\tth) \\\\\n",
    "& = \\int \\pi(\\xx_0 \\mid \\tth) \\times \\prod_{t=0}^T g(\\yy_t \\mid \\xx_t, \\tth) \\times \\prod_{t=1}^T f(\\xx_t \\mid \\xx_{t-1}, \\tth) \\ud \\xx_{0:T},   \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "but this integral is typically intractable.  The state-of-the-art for approximating it is via [particle filtering methods](https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/publications/dj11.pdf).  **PFJAX** provides several particle filters to estimate the marginal loglikelihood $\\ell(\\tth) = \\log \\Ell(\\tth)$, along with its gradient $\\nabla \\ell(\\tth) = \\frac{\\partial}{\\partial \\tth} \\ell(\\tth)$ and hessian $\\nabla^2 \\ell(\\tth) = \\frac{\\partial^2}{\\partial \\tth \\partial \\tth'} \\ell(\\tth)$.  To do this efficiently, **PFJAX** uses JIT-compilation and automatic differentiation as provided by the [**JAX**](https://github.com/google/jax) library.\n",
    "\n",
    "In this tutorial, we'll show how to use **PFJAX** to:\n",
    "\n",
    "- Create a state-space model class inheriting from `pfjax.BaseModel`.\n",
    "\n",
    "- Use this class to simulate data this state-space model via `pfjax.simulate()`.\n",
    "\n",
    "- Use a basic particle filter `pfjax.particle_filter()` to estimate the marginal loglikelihood $\\ell(\\tth)$.\n",
    "\n",
    "\n",
    "<!-- - Simulate data from a state space model and estimate its marginal loglikelihood $\\ell(\\tth)$.\n",
    "\n",
    "- Approximate the maximum likelihood estimator $\\hat \\tth = \\argmax_{\\tth} \\ell(\\tth)$ and its variance $\\var(\\hat \\tth)$ via stochastic optimization.\n",
    "\n",
    "- Calculate the posterior distribution $p(\\tth \\mid \\yy_{0:T}) \\propto \\Ell(\\tth) \\times \\pi(\\tth)$ via Markov chain Monte Carlo (MCMC) sampling. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fff96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T06:03:30.196690Z",
     "start_time": "2022-12-06T06:03:30.118503Z"
    }
   },
   "outputs": [],
   "source": [
    "# jax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import jax.random\n",
    "from functools import partial\n",
    "# plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import projplot as pjp\n",
    "# pfjax\n",
    "import pfjax as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258bfca",
   "metadata": {},
   "source": [
    "## Example Model: Brownian Motion with Drift\n",
    "\n",
    "The model is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_0 & \\sim \\N(0, \\sigma^2 \\dt) \\\\\n",
    "x_t & \\sim \\N(x_{t-1} + \\mu \\dt, \\sigma^2 \\dt) \\\\\n",
    "y_t & \\sim \\N(x_t, \\tau^2).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The parameters of the model are $\\tth = (\\mu, \\sigma, \\tau)$.  Since $(\\xx_{0:T}, \\yy_{0:T})$ are jointly normal given $\\tth$, we can show (see [here](#bm_deriv)) that $\\yy_{0:T}$ is multivariate normal with mean and covariance \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid \\tth] & = \\tilde \\mu t, \\\\ \n",
    "\\cov(y_s, y_t) & = \\tilde \\sigma^2 \\cdot (1 + \\min(s, t)) + \\tau^2 \\delta_{st},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde \\mu = \\mu \\dt$, $\\tilde \\sigma^2 = \\sigma^2 \\dt$, and $\\delta_{st}$ is the indicator function.  These formulas provide an analytic expression for $\\Ell(\\tth) = p(\\yy_{0:T} \\mid \\tth)$, which we can use to benchmark our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944e8e3",
   "metadata": {},
   "source": [
    "## Particle Filter Approximation of the Marginal Loglikelihood\n",
    "\n",
    "Using the particle filters provided by **PFJAX** requires at minimum that a class defining the state-space model be provided.  This can be done by deriving from a base class `pfjax.BaseModel` as done in the example code below.  \n",
    "\n",
    "- The methods `{prior/state/meas}_{lpdf/sample}()` and their argument inputs and outputs must be as below for the machinery of **PFJAX** to operate correctly. \n",
    "- In this case we can directly compare the particle filter loglikelihood approximation $\\hat \\ell(\\tth)$ to the true loglikelihood $\\ell(\\tth)$ defined the formulas above, so we'll also add a method `exact_lpdf()` defining $\\ell(\\tth)$ to the derived class below.\n",
    "- **PFJAX** models may optionally define methods `{init/step}_{lpdf/sample}()` as proposal distributions $q(\\xx_0 \\mid \\tth)$ and $q(\\xx_{t} \\mid \\xx_{t-1}, \\tth)$, respectively.  A common choice for these is the so-called **Bootstrap filter**, where \n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    q(\\xx_0 \\mid \\tth) & = \\pi(\\xx_0 \\mid \\tth), \\\\\n",
    "    q(\\xx_{t} \\mid \\xx_{t-1}, \\tth) &= f(\\xx_t \\mid \\xx_{t-1}, \\tth).\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    For computational efficiency this must be be explicitly specified to `pfjax.BaseModel` as we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6688ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T04:23:58.049152Z",
     "start_time": "2022-11-22T04:23:58.032347Z"
    }
   },
   "outputs": [],
   "source": [
    "class BMModel(pf.BaseModel):\n",
    "\n",
    "    def __init__(self, dt):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        Args:\n",
    "            dt: JAX scalar specifying the interobservation time.\n",
    "        \"\"\"\n",
    "        super().__init__(bootstrap=True)  # Sets up a bootstrap filter\n",
    "        self._dt = dt\n",
    "\n",
    "    def prior_lpdf(self, x_init, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the initial state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_init, loc=0.0, scale=scale)\n",
    "\n",
    "    def prior_sample(self, key, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the initial state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return scale * jax.random.normal(key=key)\n",
    "\n",
    "    def state_lpdf(self, x_curr, x_prev, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the current state variable at time `t` given the previous state variable at time `t-1`.\n",
    "        \"\"\"\n",
    "        loc = x_prev + theta[0] * self._dt\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_curr, loc=loc, scale=scale)\n",
    "\n",
    "    def state_sample(self, key, x_prev, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the current state variable at time `t` given the previous state variable at time `t-1`.\n",
    "        \"\"\"\n",
    "        loc = x_prev + theta[0] * self._dt\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def meas_lpdf(self, y_curr, x_curr, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the current measurement variable at time `t` given the current state variable at time `t`.\n",
    "        \"\"\"\n",
    "        loc = x_curr\n",
    "        scale = theta[2]\n",
    "        return jsp.stats.norm.logpdf(y_curr, loc=loc, scale=scale)\n",
    "\n",
    "    def meas_sample(self, key, x_curr, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the current measurement variable at time `t` given the current state variable at time `t`.\n",
    "        \"\"\"\n",
    "        loc = x_curr\n",
    "        scale = theta[2]\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def exact_lpdf(self, y_meas, theta):\n",
    "        \"\"\"\n",
    "        Calculate exact log-density of measurement variables at times `t=0:T`.\n",
    "        \"\"\"\n",
    "        mu_tilde = theta[0] * self._dt\n",
    "        sigma2_tilde = theta[1] * theta[1] * self._dt\n",
    "        tau2 = theta[2] * theta[2]\n",
    "        n_obs = y_meas.shape[0]  # number of observations\n",
    "        t_meas = jnp.arange(n_obs)\n",
    "        mu_y = mu_tilde * t_meas  # mean of y_meas\n",
    "        # variance of y_meas\n",
    "        Sigma_y = sigma2_tilde * \\\n",
    "            jax.vmap(lambda t: jnp.minimum(t, t_meas))(t_meas)\n",
    "        Sigma_y = Sigma_y + sigma2_tilde\n",
    "        Sigma_y = Sigma_y + tau2 * jnp.eye(n_obs)\n",
    "        return jsp.stats.multivariate_normal.logpdf(\n",
    "            x=y_meas, mean=mu_y, cov=Sigma_y\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c760f77",
   "metadata": {},
   "source": [
    "### Simulate Data\n",
    "\n",
    "This is accomplished with the function `pfjax.simulate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98bdc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T04:24:02.267755Z",
     "start_time": "2022-11-22T04:24:01.989096Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameter values\n",
    "mu = 0.\n",
    "sigma = .2\n",
    "tau = .1\n",
    "theta_true = jnp.array([mu, sigma, tau])\n",
    "\n",
    "# data specification\n",
    "dt = .5\n",
    "n_obs = 100\n",
    "x_init = jnp.array(0.)\n",
    "\n",
    "# initial key for random numbers\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# simulate data\n",
    "bm_model = BMModel(dt=dt)\n",
    "key, subkey = jax.random.split(key)\n",
    "y_meas, x_state = pf.simulate(\n",
    "    model=bm_model,\n",
    "    key=subkey,\n",
    "    n_obs=n_obs,\n",
    "    x_init=x_init,\n",
    "    theta=theta_true\n",
    ")\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"x_state\": jnp.squeeze(x_state),\n",
    "                         \"y_meas\": jnp.squeeze(y_meas)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c1069",
   "metadata": {},
   "source": [
    "### Calculate Particle Filter Marginal Logikelihood\n",
    "\n",
    "For this we will use the basic particle filter provided by `pfjax.particle_filter()`.  We'll also compare the particle filter approximation $\\hat{\\ell}(\\tth)$ to the exact loglikelihood $\\ell(\\tth)$ using \"projection plots\", i.e., we'll plot the one-dimensional marginal loglikelihood in each of the parameters $\\tth = (\\mu, \\sigma, \\tau)$ with the other two parameters fixed at their simulated values.  \n",
    "\n",
    "- Projection plots are obtained with the help of the **projplot** package.\n",
    "- Projection plots involve a fairly large number of evaluations of $\\hat{\\ell}(\\tth)$, which itself involes $\\bO(TN)$ evaluations of the state-space model functions $f(\\xx_t \\mid \\xx_{t-1}, \\tth)$ and $g(\\yy_t \\mid \\xx_t, \\tth)$.  We will therefore employ the JIT compilation engine offered by **JAX** to massively speed up function evaluations.\n",
    "- We'll first create helper functions to evaluate the loglikelihoods which vectorize over multiple parameter values at once.  In particular, this allows us to easily specify a different PRNG key for each evaluation of $\\hat{\\ell}(\\tth)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c0d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T04:24:15.670397Z",
     "start_time": "2022-11-22T04:24:07.827758Z"
    }
   },
   "outputs": [],
   "source": [
    "# particle filter specification\n",
    "n_particles = 200  # number of particles\n",
    "\n",
    "\n",
    "def bm_loglik_pf_nojit(theta, key):\n",
    "    \"\"\"\n",
    "    Particle filter loglikelihood of the BM model (un-jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    subkeys = jax.random.split(key, num=theta.shape[0])\n",
    "    return jax.vmap(lambda _theta, _key: pf.particle_filter(\n",
    "        model=bm_model,\n",
    "        key=_key,\n",
    "        y_meas=y_meas,\n",
    "        n_particles=n_particles,\n",
    "        theta=_theta,\n",
    "        history=False,\n",
    "        score=False,\n",
    "        fisher=False\n",
    "    )[\"loglik\"])(theta, subkeys)\n",
    "\n",
    "\n",
    "# jitted version\n",
    "bm_loglik_pf_jit = jax.jit(bm_loglik_pf_nojit)\n",
    "\n",
    "# check jit speedup\n",
    "key, subkey = jax.random.split(key)\n",
    "%timeit bm_loglik_pf_nojit(theta=theta_true, key=subkey)\n",
    "%timeit bm_loglik_pf_jit(theta=theta_true, key=subkey)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def bm_loglik_exact(theta):\n",
    "    \"\"\"\n",
    "    Exact loglikelihood of the BM model (jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    return jax.vmap(lambda _theta: bm_model.exact_lpdf(\n",
    "        theta=_theta,\n",
    "        y_meas=y_meas\n",
    "    ))(theta)\n",
    "\n",
    "# projection plot specification\n",
    "n_pts = 100  # number of evaluation points per plot\n",
    "# plot limits for each parameter\n",
    "theta_lims = jnp.array([[-.5, .5], [.1, .4], [.05, .2]])\n",
    "theta_names = [\"mu\", \"sigma\", \"tau\"]  # parameter names\n",
    "\n",
    "# calculate projection plot for exact loglikelihood\n",
    "df_exact = pjp.proj_plot(\n",
    "    fun=bm_loglik_exact,\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# calculate projection plot for particle filter loglikelihood\n",
    "df_pf = pjp.proj_plot(\n",
    "    fun=partial(bm_loglik_pf_jit, key=subkey),\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# merge data frames and plot them\n",
    "plot_df = pd.concat([df_exact, df_pf], ignore_index=True)\n",
    "plot_df[\"type\"] = np.repeat([\"exact\", \"pf\"], len(df_exact[\"variable\"]))\n",
    "plot_df = plot_df.rename(columns={\"y\": \"loglikelihood\"})\n",
    "rp = sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"x\", y=\"loglikelihood\",\n",
    "    hue=\"type\",\n",
    "    col=\"variable\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "rp.set_titles(col_template=\"{col_name}\")\n",
    "rp.set(xlabel=None)\n",
    "# add true parameter values\n",
    "for ax, theta in zip(rp.axes.flat, theta_true):\n",
    "    ax.axvline(theta, linestyle=\"--\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474f5e5",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- Jitting the particle filter computation offers almost a 1000x speed improvement!\n",
    "- We see that the particle filter loglikelihood approximation is very accurate.  However, it seems to be increasingly biased as we move away from the true parameter value (dashed vertical lines).  This is likely because the quality of the bootstrap filter deteriorates considerably when this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a49f2",
   "metadata": {},
   "source": [
    "## Ideal Proposal with Noise (Non-Bootstrap Particle Filter)\n",
    "\n",
    "Generally, the **ideal proposal** for the BM model is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q(x_0 \\mid \\tth) & = p(x_0 \\mid y_0, \\tth) = \\frac{p(x_0, y_0 \\mid \\tth)}{p(y_0 \\mid \\tth)} = \\frac{\\pi(x_0 \\mid \\tth) p(y_0 \\mid x_0, \\tth)}{p(y_0 \\mid \\tth)}, \\\\\n",
    "q(x_{t} \\mid x_{t-1}, \\tth) &= p(x_t \\mid x_{t-1}, y_t, \\tth) = \\frac{p(x_t, y_t \\mid x_{t-1}, \\tth)}{p(y_t \\mid x_{t-1}, \\tth)} = \\frac{p(y_t \\mid x_t, \\tth) p(x_t \\mid x_{t-1}, \\tth)}{p(y_t \\mid \\tth)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p(y_t \\mid x_{t-1}, \\tth) = p(y_t \\mid \\tth)$ is due to the Markov property of the state-space model.\n",
    "\n",
    "For the BM model we know the analytic expression of $p(y_t \\mid \\tth)$, which is a Normal distribution with mean and variance given by $\\tilde \\mu t$ and $\\tilde \\sigma^2 (1 + t) + \\tau^2$, respectively (see [here](#bm_deriv) for a general derivation).\n",
    "\n",
    "$p(y_t \\mid x_{t-1}, \\tth)$ is just a normalizing constant which should not essentially affect our proposal sampling of $x_t$ at each step.\n",
    "\n",
    "Then we can set a **non-Bootstrap particle filter** using the ideal proposal perturbed by a systematic noise $\\eps$ or a tiny independent random noise $\\eps\\N(0,1)$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q(\\xx_0 \\mid \\tth) & = \\frac{\\N(0, \\sigma^2 \\dt)\\N(x_0, \\tau^2)}{p(y_0 \\mid \\tth)} + \\N(0, \\eps^2),\\\\\n",
    "q(\\xx_{t} \\mid \\xx_{t-1}, \\tth) &= \\frac{\\N(x_t, \\tau^2) \\N(x_{t-1} + \\mu \\dt, \\sigma^2 \\dt)}{p(y_t \\mid \\tth)} + \\N(0, \\eps^2) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We notice that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\N(x_t, \\tau^2) \\N(x_{t-1} + \\mu \\dt, \\sigma^2 \\dt)}{p(y_t \\mid \\tth)} \n",
    "&\\propto \\exp\\left( -\\frac{(y_t - x_t)^2}{2\\tau^2} - \\frac{(x_t - x_{t-1} - \\mu\\dt)^2}{2\\sigma^2\\dt}  \\right) \\\\ \n",
    "&\\propto \\exp\\left(-\\left(\\frac{1}{2\\tau^2} + \\frac{1}{2\\sigma^2\\dt} \\right)x_t^2 + \\left(\\frac{y_t}{\\tau^2} + \\frac{x_{t-1} + \\mu\\dt}{\\sigma^2\\dt} \\right)x_t + C \\right) \\\\\n",
    "&\\propto \\exp\\left(-A\\left(x_t - \\frac{B}{2A} \\right)^2\\right) = \\N\\left(\\frac{B}{2A}, \\frac{1}{2A} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $A = \\frac{1}{2\\tau^2} + \\frac{1}{2\\sigma^2\\dt}$, $B = \\frac{y_t}{\\tau^2} + \\frac{x_{t-1} + \\mu\\dt}{\\sigma^2\\dt}$. Similarly,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\N(0, \\sigma^2 \\dt)\\N(x_0, \\tau^2)}{p(y_0 \\mid \\tth)} &\\propto \\exp\\left(-\\left(\\frac{1}{2\\tau^2} + \\frac{1}{2\\sigma^2\\dt}\\right) x_0^2 + \\frac{y_0}{\\tau^2} x_0 \\right) \\\\\n",
    "&\\propto \\exp\\left(-A_0 \\left(x_0 - \\frac{B_0}{2A_0}\\right)^2 \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $A_0 = \\frac{1}{2\\tau^2} + \\frac{1}{2\\sigma^2\\dt}$, $B_0 = \\frac{y_0}{\\tau^2}$.\n",
    "\n",
    "This relationship reveals how to sample $x_t$ at each step from the ideal proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc77c7e",
   "metadata": {},
   "source": [
    "### Two ways to define a customized PF\n",
    "\n",
    "Here we provide two ways to customize the BM model with the ideal proposal.\n",
    "\n",
    "   1. Define individually the following member functions within the BM model class:\n",
    "       - `init_{lpdf/sample}` for $x_0$ to calculate the log probability density / to draw the state varible from the initial proposal at time `t=0`. \n",
    "       - `step_{lpdf/sample}` for $x_t$ (`t=1,2,...`) to calculate the log probability density of the proposal distribution / to sample from the proposal.\n",
    "       \n",
    "   \n",
    "   2. Define `pf_{init/step}` function in the BM model class (which overrides the same function in the base model class) to calculate the initial / general step of particle filter update, i.e. to return a draw from the proposal distribution.\n",
    "   \n",
    "Now the first way to define the model is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efd41a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T05:38:56.122719Z",
     "start_time": "2022-12-06T05:38:56.093256Z"
    }
   },
   "outputs": [],
   "source": [
    "class BMModel_nBPF(pf.BaseModel):\n",
    "\n",
    "    def __init__(self, dt):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        Args:\n",
    "            dt: JAX scalar specifying the interobservation time.\n",
    "            err: additional error term to the ideal proposal\n",
    "        \"\"\"\n",
    "        super().__init__(bootstrap=False)  # Sets up a non-bootstrap filter\n",
    "        self._dt = dt\n",
    "#         self._err = err\n",
    "        \n",
    "    def init_lpdf(self, x_init, y_init, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the initial proposal for the state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "#         scale = theta[1] * jnp.sqrt(self._dt)\n",
    "#         return jnp.log(jsp.stats.norm.pdf(x_init, loc=0.0, scale=scale) + self._err)\n",
    "        A = 1/(2.0 * theta[2] * theta[2]) + 1/(2.0 * theta[1] * theta[1] * self._dt)\n",
    "        B = y_init / (theta[2] * theta[2])\n",
    "        return jsp.stats.norm.logpdf(x_init, loc=B/(2.0*A), scale=jnp.sqrt(1/(2.0 * A)))\n",
    "        \n",
    "    def init_sample(self, key, y_init, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw state variable from the initial proposal distribution at time `t=0`.\n",
    "        \"\"\"\n",
    "#         scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        A = 1/(2.0 * theta[2] * theta[2]) + 1/(2.0 * theta[1] * theta[1] * self._dt)\n",
    "        B = y_init / (theta[2] * theta[2])\n",
    "#         return scale * jax.random.normal(key=key) + self._err * jax.random.normal(key=2*key)\n",
    "        return B/(2.0 * A) + jnp.sqrt(1/(2.0 * A)) * jax.random.normal(key=key) #+ self._err * jax.random.normal(key=2*key)\n",
    "    \n",
    "    def step_lpdf(self, x_curr, x_prev, y_curr, theta):\n",
    "        \"\"\"\n",
    "        Calculate log-density of the proposal distribution\n",
    "        \"\"\"\n",
    "#         loc_prev = x_prev + theta[0] * self._dt\n",
    "#         scale = theta[1] * jnp.sqrt(self._dt)\n",
    "#         lpdf_ans = jsp.stats.norm.logpdf(x_curr, loc = loc_prev, scale = scale)\n",
    "#         lpdf_ans += jsp.stats.norm.logpdf(y_curr, loc = x_curr, scale = theta[2])\n",
    "#         sigma2_tilde = theta[1] * theta[1] * self._dt\n",
    "#         tau2 = theta[2] * theta[2]\n",
    "#         lpdf_ans -= jsp.stats.norm.logpdf(y_curr, loc = loc_prev, scale = jnp.sqrt(sigma2_tilde + tau2))\n",
    "        A = 1/(2.0 * theta[2] * theta[2]) + 1/(2.0 * theta[1] * theta[1] * self._dt)\n",
    "        B = y_curr / (theta[2] * theta[2]) + (x_prev + theta[0] * self._dt) / (theta[1] * theta[1] * self._dt)\n",
    "        lpdf_ans = jsp.stats.norm.logpdf(x_curr, loc = B/(2.0 * A), scale = jnp.sqrt(1/(2.0 * A)))\n",
    "#         return jnp.log(jnp.exp(lpdf_ans) + self._err) # self._err only adds constant to the logpdf\n",
    "        return lpdf_ans\n",
    "    \n",
    "    def step_sample(self, key, x_prev, y_curr, theta):\n",
    "        \"\"\"\n",
    "        Sample from the proposal distribution\n",
    "        \"\"\"\n",
    "        A = 1/(2.0 * theta[2] * theta[2]) + 1/(2.0 * theta[1] * theta[1] * self._dt)\n",
    "        B = y_curr / (theta[2] * theta[2]) + (x_prev + theta[0] * self._dt) / (theta[1] * theta[1] * self._dt)\n",
    "        return B/(2.0 * A) + jnp.sqrt(1/(2.0 * A)) * jax.random.normal(key = key) #+ self._err * jax.random.normal(key=2*key)\n",
    "\n",
    "    def prior_lpdf(self, x_init, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the initial state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_init, loc=0.0, scale=scale)\n",
    "\n",
    "    def prior_sample(self, key, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the initial state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return scale * jax.random.normal(key=key)\n",
    "\n",
    "    def state_lpdf(self, x_curr, x_prev, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the current state variable at time `t` given the previous state variable at time `t-1`.\n",
    "        \"\"\"\n",
    "        loc = x_prev + theta[0] * self._dt\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_curr, loc=loc, scale=scale)\n",
    "\n",
    "    def state_sample(self, key, x_prev, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the current state variable at time `t` given the previous state variable at time `t-1`.\n",
    "        \"\"\"\n",
    "        loc = x_prev + theta[0] * self._dt\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def meas_lpdf(self, y_curr, x_curr, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the current measurement variable at time `t` given the current state variable at time `t`.\n",
    "        \"\"\"\n",
    "        loc = x_curr\n",
    "        scale = theta[2]\n",
    "        return jsp.stats.norm.logpdf(y_curr, loc=loc, scale=scale)\n",
    "\n",
    "    def meas_sample(self, key, x_curr, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the current measurement variable at time `t` given the current state variable at time `t`.\n",
    "        \"\"\"\n",
    "        loc = x_curr\n",
    "        scale = theta[2]\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def exact_lpdf(self, y_meas, theta):\n",
    "        \"\"\"\n",
    "        Calculate exact log-density of measurement variables at times `t=0:T`.\n",
    "        \"\"\"\n",
    "        mu_tilde = theta[0] * self._dt\n",
    "        sigma2_tilde = theta[1] * theta[1] * self._dt\n",
    "        tau2 = theta[2] * theta[2]\n",
    "        n_obs = y_meas.shape[0]  # number of observations\n",
    "        t_meas = jnp.arange(n_obs)\n",
    "        mu_y = mu_tilde * t_meas  # mean of y_meas\n",
    "        # variance of y_meas\n",
    "        Sigma_y = sigma2_tilde * \\\n",
    "            jax.vmap(lambda t: jnp.minimum(t, t_meas))(t_meas)\n",
    "        Sigma_y = Sigma_y + sigma2_tilde\n",
    "        Sigma_y = Sigma_y + tau2 * jnp.eye(n_obs)\n",
    "        return jsp.stats.multivariate_normal.logpdf(\n",
    "            x=y_meas, mean=mu_y, cov=Sigma_y\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536748d",
   "metadata": {},
   "source": [
    "Then the second way to define the same model is given below. For the ideal proposal PF,\n",
    "\n",
    "   - At `t=0`, \n",
    "   $$\n",
    "   w_0 = \\frac{\\pi(x_0 \\mid \\theta)p(y_0 \\mid x_0, \\theta)}{q(x_0 \\mid \\theta)} = p(y_0 \\mid \\theta) \\propto 1.\n",
    "   $$\n",
    "    \n",
    "   - At `t=1,2,...`, \n",
    "   $$\n",
    "   w_t = \\frac{p(y_t \\mid x_t, \\theta) p(x_t \\mid \\tilde{x}_{t-1}, \\theta)}{q(x_t \\mid \\tilde{x}_{t-1}, \\theta)}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d84f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T05:38:58.907304Z",
     "start_time": "2022-12-06T05:38:58.888405Z"
    }
   },
   "outputs": [],
   "source": [
    "class BMModel_nBPF2(pf.BaseModel):\n",
    "\n",
    "    def __init__(self, dt):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        Args:\n",
    "            dt: JAX scalar specifying the interobservation time.\n",
    "        \"\"\"\n",
    "        super().__init__(bootstrap=False)  # Sets up a non-bootstrap filter\n",
    "        self._dt = dt\n",
    "    \n",
    "    def exact_lpdf(self, y_meas, theta):\n",
    "        \"\"\"\n",
    "        Calculate exact log-density of measurement variables at times `t=0:T`.\n",
    "        \"\"\"\n",
    "        mu_tilde = theta[0] * self._dt\n",
    "        sigma2_tilde = theta[1] * theta[1] * self._dt\n",
    "        tau2 = theta[2] * theta[2]\n",
    "        n_obs = y_meas.shape[0]  # number of observations\n",
    "        t_meas = jnp.arange(n_obs)\n",
    "        mu_y = mu_tilde * t_meas  # mean of y_meas\n",
    "        # variance of y_meas\n",
    "        Sigma_y = sigma2_tilde * \\\n",
    "            jax.vmap(lambda t: jnp.minimum(t, t_meas))(t_meas)\n",
    "        Sigma_y = Sigma_y + sigma2_tilde\n",
    "        Sigma_y = Sigma_y + tau2 * jnp.eye(n_obs)\n",
    "        return jsp.stats.multivariate_normal.logpdf(\n",
    "            x=y_meas, mean=mu_y, cov=Sigma_y\n",
    "        )\n",
    "    \n",
    "    def pf_init(self, key, y_init, theta):\n",
    "        r\"\"\"\n",
    "        Initial step of particle filter.\n",
    "        Returns a draw from the proposal distribution\n",
    "        ::\n",
    "            x_init ~ q(x_init) = p(x_init | y_init, theta)\n",
    "        and calculates the log weight\n",
    "        ::\n",
    "            logw = log p(y_init | x_init, theta) + log p(x_init | theta) - log q(x_init)\n",
    "        Args:\n",
    "            key: PRNG key.\n",
    "            y_init: Measurement variable at initial time `t = 0`.\n",
    "            theta: Parameter value.\n",
    "        Returns:\n",
    "            Tuple:\n",
    "            - **x_init** - A sample from the proposal distribution at initial tme `t = 0`.\n",
    "            - **logw** - The log-weight of `x_init`.\n",
    "        \"\"\"\n",
    "        A = 1/(2.0 * theta[2] * theta[2]) + 1/(2.0 * theta[1] * theta[1] * self._dt)\n",
    "        B = y_init / (theta[2] * theta[2])\n",
    "        x_init = B/(2.0 * A) + jnp.sqrt(1/(2.0 * A)) * jax.random.normal(key=key)\n",
    "        logw = jsp.stats.norm.logpdf(y_curr, loc=x_init, scale=theta[2]) \n",
    "        logw += jsp.stats.norm.logpdf(x_init, loc=0.0, scale=theta[1] * jnp.sqrt(self._dt))\n",
    "        logw -= jsp.stats.norm.logpdf(x_init, loc=B/(2.0*A), scale=jnp.sqrt(1/(2.0 * A)))\n",
    "        return x_init, logw\n",
    "        \n",
    "        \n",
    "    def pf_step(self, key, x_prev, y_curr, theta):\n",
    "        r\"\"\"\n",
    "        Particle filter update.\n",
    "        Returns a draw from proposal distribution\n",
    "        ::\n",
    "            x_curr ~ q(x_curr) = q(x_curr | x_prev, y_curr, theta)\n",
    "        and the log weight\n",
    "        ::\n",
    "            logw = log p(y_curr | x_curr, theta) + log p(x_curr | x_prev, theta) - log q(x_curr)\n",
    "        Args:\n",
    "            key: PRNG key.\n",
    "            x_prev: State variable at previous time `t-1`.\n",
    "            y_curr: Measurement variable at current time `t`.\n",
    "            theta: Parameter value.\n",
    "        Returns:\n",
    "            Tuple:\n",
    "            - **x_curr** - A sample from the proposal distribution at current time `t`.\n",
    "            - **logw** - The log-weight of `x_curr`.\n",
    "        \"\"\"\n",
    "        A = 1/(2.0 * theta[2] * theta[2]) + 1/(2.0 * theta[1] * theta[1] * self._dt)\n",
    "        B = y_curr / (theta[2] * theta[2]) + (x_prev + theta[0] * self._dt) / (theta[1] * theta[1] * self._dt)\n",
    "        x_curr = B/(2.0 * A) + jnp.sqrt(1/(2.0 * A)) * jax.random.normal(key = key) \n",
    "        logw = jsp.stats.norm.logpdf(y_curr, loc=x_curr, scale=theta[2])\n",
    "        logw += jsp.stats.norm.logpdf(x_curr, loc=x_prev + theta[0] * self._dt, scale=theta[1] * jnp.sqrt(self._dt))\n",
    "        lp_prop = jsp.stats.norm.logpdf(x_curr, loc = B/(2.0 * A), scale = jnp.sqrt(1/(2.0 * A)))\n",
    "        logw -= lp_prop\n",
    "        return x_curr, logw\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cc8b3",
   "metadata": {},
   "source": [
    "### Simulate data\n",
    "\n",
    "We can then simulate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d014d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T05:39:06.743595Z",
     "start_time": "2022-12-06T05:39:06.453491Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameter values\n",
    "mu = 0.\n",
    "sigma = .2\n",
    "tau = .1\n",
    "theta_true = jnp.array([mu, sigma, tau])\n",
    "\n",
    "# data specification\n",
    "# err = 0.0\n",
    "dt = .5\n",
    "n_obs = 100\n",
    "x_init = jnp.array(0.)\n",
    "\n",
    "# initial key for random numbers\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# simulate data\n",
    "bm_model_nbpf = BMModel_nBPF(dt=dt)\n",
    "key, subkey = jax.random.split(key)\n",
    "y_meas, x_state = pf.simulate(\n",
    "    model=bm_model_nbpf,\n",
    "    key=subkey,\n",
    "    n_obs=n_obs,\n",
    "    x_init=x_init,\n",
    "    theta=theta_true\n",
    ")\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"x_state\": jnp.squeeze(x_state),\n",
    "                         \"y_meas\": jnp.squeeze(y_meas)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a8fad",
   "metadata": {},
   "source": [
    "### Calculate Particle Filter Marginal Logikelihood\n",
    "\n",
    "For this we will use the built-in particle filter provided by `pfjax.particle_filter()`.  We'll also compare the particle filter approximation $\\hat{\\ell}(\\tth)$ to the exact loglikelihood $\\ell(\\tth)$ using \"projection plots\", i.e., we'll plot the one-dimensional marginal loglikelihood in each of the parameters $\\tth = (\\mu, \\sigma, \\tau)$ with the other two parameters fixed at their simulated values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99475a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T05:39:33.588777Z",
     "start_time": "2022-12-06T05:39:25.107296Z"
    }
   },
   "outputs": [],
   "source": [
    "# particle filter specification\n",
    "n_particles = 200  # number of particles\n",
    "\n",
    "\n",
    "def bm_loglik_pf_nojit(theta, key):\n",
    "    \"\"\"\n",
    "    Particle filter loglikelihood of the BM model (un-jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    subkeys = jax.random.split(key, num=theta.shape[0])\n",
    "    return jax.vmap(lambda _theta, _key: pf.particle_filter(\n",
    "        model=bm_model_nbpf,\n",
    "        key=_key,\n",
    "        y_meas=y_meas,\n",
    "        n_particles=n_particles,\n",
    "        theta=_theta,\n",
    "        history=False,\n",
    "        score=False,\n",
    "        fisher=False\n",
    "    )[\"loglik\"])(theta, subkeys)\n",
    "\n",
    "\n",
    "# jitted version\n",
    "bm_loglik_pf_jit = jax.jit(bm_loglik_pf_nojit)\n",
    "\n",
    "# check jit speedup\n",
    "key, subkey = jax.random.split(key)\n",
    "%timeit bm_loglik_pf_nojit(theta=theta_true, key=subkey)\n",
    "%timeit bm_loglik_pf_jit(theta=theta_true, key=subkey)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def bm_loglik_exact(theta):\n",
    "    \"\"\"\n",
    "    Exact loglikelihood of the BM model (jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    return jax.vmap(lambda _theta: bm_model_nbpf.exact_lpdf(\n",
    "        theta=_theta,\n",
    "        y_meas=y_meas\n",
    "    ))(theta)\n",
    "\n",
    "# projection plot specification\n",
    "n_pts = 100  # number of evaluation points per plot\n",
    "# plot limits for each parameter\n",
    "theta_lims = jnp.array([[-.5, .5], [.1, .4], [.05, .2]])\n",
    "theta_names = [\"mu\", \"sigma\", \"tau\"]  # parameter names\n",
    "\n",
    "# calculate projection plot for exact loglikelihood\n",
    "df_exact = pjp.proj_plot(\n",
    "    fun=bm_loglik_exact,\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# calculate projection plot for particle filter loglikelihood\n",
    "df_pf = pjp.proj_plot(\n",
    "    fun=partial(bm_loglik_pf_jit, key=subkey),\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# merge data frames and plot them\n",
    "plot_df = pd.concat([df_exact, df_pf], ignore_index=True)\n",
    "plot_df[\"type\"] = np.repeat([\"exact\", \"pf\"], len(df_exact[\"variable\"]))\n",
    "plot_df = plot_df.rename(columns={\"y\": \"loglikelihood\"})\n",
    "rp = sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"x\", y=\"loglikelihood\",\n",
    "    hue=\"type\",\n",
    "    col=\"variable\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "rp.set_titles(col_template=\"{col_name}\")\n",
    "rp.set(xlabel=None)\n",
    "# add true parameter values\n",
    "for ax, theta in zip(rp.axes.flat, theta_true):\n",
    "    ax.axvline(theta, linestyle=\"--\", color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe4fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T05:39:38.229129Z",
     "start_time": "2022-12-06T05:39:37.628371Z"
    }
   },
   "outputs": [],
   "source": [
    "bm_model_nbpf2 = BMModel_nBPF2(dt=dt)\n",
    "\n",
    "def bm_loglik_pf_nojit2(theta, key):\n",
    "    \"\"\"\n",
    "    Particle filter loglikelihood of the BM model (un-jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    subkeys = jax.random.split(key, num=theta.shape[0])\n",
    "    return jax.vmap(lambda _theta, _key: pf.particle_filter(\n",
    "        model=bm_model_nbpf2,\n",
    "        key=_key,\n",
    "        y_meas=y_meas,\n",
    "        n_particles=n_particles,\n",
    "        theta=_theta,\n",
    "        history=False,\n",
    "        score=False,\n",
    "        fisher=False\n",
    "    )[\"loglik\"])(theta, subkeys)\n",
    "\n",
    "\n",
    "# jitted version\n",
    "bm_loglik_pf_jit2 = jax.jit(bm_loglik_pf_nojit2)\n",
    "\n",
    "@jax.jit\n",
    "def bm_loglik_exact2(theta):\n",
    "    \"\"\"\n",
    "    Exact loglikelihood of the BM model (jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    return jax.vmap(lambda _theta: bm_model_nbpf2.exact_lpdf(\n",
    "        theta=_theta,\n",
    "        y_meas=y_meas\n",
    "    ))(theta)\n",
    "\n",
    "# projection plot specification\n",
    "n_pts = 100  # number of evaluation points per plot\n",
    "# plot limits for each parameter\n",
    "theta_lims = jnp.array([[-.5, .5], [.1, .4], [.05, .2]])\n",
    "theta_names = [\"mu\", \"sigma\", \"tau\"]  # parameter names\n",
    "\n",
    "# calculate projection plot for exact loglikelihood\n",
    "# df_exact2 = pjp.proj_plot(\n",
    "#     fun=bm_loglik_exact2,\n",
    "#     x_opt=theta_true,\n",
    "#     x_lims=theta_lims,\n",
    "#     x_names=theta_names,\n",
    "#     n_pts=n_pts,\n",
    "#     vectorized=True,\n",
    "#     plot=False\n",
    "# )\n",
    "\n",
    "# calculate projection plot for particle filter loglikelihood\n",
    "df_pf2 = pjp.proj_plot(\n",
    "    fun=partial(bm_loglik_pf_jit2, key=subkey),\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# merge data frames and plot them\n",
    "plot_df2 = pd.concat([df_exact, df_pf2], ignore_index=True)\n",
    "plot_df2[\"type\"] = np.repeat([\"exact\", \"pf\"], len(df_exact[\"variable\"]))\n",
    "plot_df2 = plot_df2.rename(columns={\"y\": \"loglikelihood\"})\n",
    "rp = sns.relplot(\n",
    "    data=plot_df2, kind=\"line\",\n",
    "    x=\"x\", y=\"loglikelihood\",\n",
    "    hue=\"type\",\n",
    "    col=\"variable\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "rp.set_titles(col_template=\"{col_name}\")\n",
    "rp.set(xlabel=None)\n",
    "# add true parameter values\n",
    "for ax, theta in zip(rp.axes.flat, theta_true):\n",
    "    ax.axvline(theta, linestyle=\"--\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713c259",
   "metadata": {},
   "source": [
    "### Old: comparison of different error terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6650992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T04:41:44.895865Z",
     "start_time": "2022-12-06T04:41:44.754314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bm_model_nbpf_err1 = BMModel_nBPF(dt=dt, err=0.05) # err1\n",
    "\n",
    "bm_model_nbpf_err2 = BMModel_nBPF(dt=dt, err=0.1) # err2\n",
    "\n",
    "def bm_loglik_pf_nojit_err1(theta, key):\n",
    "    \"\"\"\n",
    "    Particle filter loglikelihood of the BM model (un-jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    subkeys = jax.random.split(key, num=theta.shape[0])\n",
    "    return jax.vmap(lambda _theta, _key: pf.particle_filter(\n",
    "        model=bm_model_nbpf_err1,\n",
    "        key=_key,\n",
    "        y_meas=y_meas,\n",
    "        n_particles=n_particles,\n",
    "        theta=_theta,\n",
    "        history=False,\n",
    "        score=False,\n",
    "        fisher=False\n",
    "    )[\"loglik\"])(theta, subkeys)\n",
    "\n",
    "def bm_loglik_pf_nojit_err2(theta, key):\n",
    "    \"\"\"\n",
    "    Particle filter loglikelihood of the BM model (un-jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    subkeys = jax.random.split(key, num=theta.shape[0])\n",
    "    return jax.vmap(lambda _theta, _key: pf.particle_filter(\n",
    "        model=bm_model_nbpf_err2,\n",
    "        key=_key,\n",
    "        y_meas=y_meas,\n",
    "        n_particles=n_particles,\n",
    "        theta=_theta,\n",
    "        history=False,\n",
    "        score=False,\n",
    "        fisher=False\n",
    "    )[\"loglik\"])(theta, subkeys)\n",
    "\n",
    "# jitted version\n",
    "bm_loglik_pf_jit_err1 = jax.jit(bm_loglik_pf_nojit_err1)\n",
    "\n",
    "bm_loglik_pf_jit_err2 = jax.jit(bm_loglik_pf_nojit_err2)\n",
    "\n",
    "# key, subkey = jax.random.split(key)\n",
    "\n",
    "# projection plot specification\n",
    "n_pts = 100  # number of evaluation points per plot\n",
    "# plot limits for each parameter\n",
    "theta_lims = jnp.array([[-.5, .5], [.1, .4], [.05, .2]])\n",
    "theta_names = [\"mu\", \"sigma\", \"tau\"]  # parameter names\n",
    "\n",
    "# calculate projection plot for exact loglikelihood\n",
    "# which is the same as no error model\n",
    "\n",
    "# calculate projection plot for particle filter loglikelihood\n",
    "df_pf_err1 = pjp.proj_plot(\n",
    "    fun=partial(bm_loglik_pf_jit_err1, key=subkey),\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "df_pf_err2 = pjp.proj_plot(\n",
    "    fun=partial(bm_loglik_pf_jit_err2, key=subkey),\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# merge data frames and plot them\n",
    "plot_df = pd.concat([df_exact, df_pf_err1, df_pf_err2], ignore_index=True)\n",
    "plot_df[\"type\"] = np.repeat([\"exact\", \"pf_err1\", \"pf_err2\"], len(df_exact[\"variable\"]))\n",
    "plot_df = plot_df.rename(columns={\"y\": \"loglikelihood\"})\n",
    "rp = sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"x\", y=\"loglikelihood\",\n",
    "    hue=\"type\",\n",
    "    col=\"variable\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "rp.set_titles(col_template=\"{col_name}\")\n",
    "rp.set(xlabel=None)\n",
    "# add true parameter values\n",
    "for ax, theta in zip(rp.axes.flat, theta_true):\n",
    "    ax.axvline(theta, linestyle=\"--\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41d92b",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- How to set up a customized proposal\n",
    "\n",
    "- Different ways to set up particle filter (init_sample, step_sample, etc. vs pf_step function)\n",
    "\n",
    "- For $x_0$: define init_sample/lpdf\n",
    "\n",
    "- \n",
    "\n",
    "- Just compare the bootstrap PF plot and the ideal proposal plot\n",
    "\n",
    "\n",
    "### TODO(OLD)\n",
    "\n",
    " - Compare different $\\eps$ settings in an overlapped projection plot.\n",
    " \n",
    " - If time is allowed, we may later consider a nonlinear dynamic model\n",
    " \n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     X_n &= \\alpha X_{n-1} + \\beta \\frac{X_{n-1}}{1 + X_{n-1}^2} + \\gamma \\cos(\\omega n) + \\sigma_x U_n, \\\\\n",
    "     Y_n &= \\eta X_n^2 + \\sigma_y V_n\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "     \n",
    "     where $U_n, V_n \\iid \\N(0,1)$.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d32618",
   "metadata": {},
   "source": [
    "## OU Model with Measurement Errors (Another non-Bootstrap PF model)\n",
    "\n",
    "Suppose the model is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_0 & \\sim \\N(0, \\sigma^2 \\dt) \\\\\n",
    "x_t & \\sim \\N(x_{t-1} - \\mu x_{t-1} \\dt, \\sigma^2 \\dt) \\\\\n",
    "y_t & \\sim \\N(x_t, \\tau^2).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The parameters of the model are $\\tth = (\\mu, \\sigma, \\tau)$ with $\\mu, \\sigma, \\tau > 0$.  Since $(\\xx_{0:T}, \\yy_{0:T})$ are jointly normal given $\\tth$, we can show that $\\yy_{0:T}$ is multivariate normal with mean and covariance \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid \\tth] & = E[E[x_t \\mid x_0, \\tth]] = E[x_0 e^{-\\tilde\\mu t}] = 0, \\\\ \n",
    "\\cov(y_s, y_t) & = \\frac{\\sigma^2}{2\\mu} e^{-\\tilde\\mu|t-s|} + \\tau^2 \\delta_{st},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde \\mu = \\mu \\dt$, and $\\delta_{st}$ is the indicator function.  These formulas provide an analytic expression for $\\Ell(\\tth) = p(\\yy_{0:T} \\mid \\tth)$, which we can use to benchmark our calculations.\n",
    "\n",
    "For a **non-Bootstrap particle filter**, we can set proposal distribution $q(\\xx_0 \\mid \\tth)$ and $q(\\xx_{t} \\mid \\xx_{t-1}, \\tth)$, respectively as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q(\\xx_0 \\mid \\tth) & = \\N(0, 1), \\\\\n",
    "q(\\xx_{t} \\mid \\xx_{t-1}, \\tth) &= \\N(x_{t-1}, \\sigma^2 \\dt).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Clearly, the non-Bootstrap PF proposal is very naive, and may not be the optimal in terms of minimizing the predication variance. We should expect more number of particles needed to better fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840c58d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T15:23:09.083478Z",
     "start_time": "2022-11-15T15:23:09.061201Z"
    }
   },
   "outputs": [],
   "source": [
    "class OUModel(pf.BaseModel):\n",
    "\n",
    "    def __init__(self, dt):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        Args:\n",
    "            dt: JAX scalar specifying the interobservation time.\n",
    "        \"\"\"\n",
    "#         super().__init__(bootstrap=True)  # Sets up a bootstrap filter\n",
    "        super().__init__(bootstrap=False)  # Sets up a non-bootstrap filter\n",
    "        self._dt = dt\n",
    "        \n",
    "    def init_lpdf(self, x_init, y_init, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the initial proposal for the state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        return jsp.stats.norm.logpdf(x_init, loc=0.0, scale=1.0)\n",
    "        \n",
    "    def init_sample(self, key, y_init, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw state variable from the initial proposal distribution at time `t=0`.\n",
    "        \"\"\"\n",
    "        return 1.0 * jax.random.normal(key=key)\n",
    "    \n",
    "    def step_lpdf(self, x_curr, x_prev, y_curr, theta):\n",
    "        \"\"\"\n",
    "        Calculate log-density of the proposal distribution\n",
    "        \"\"\"\n",
    "        loc = x_prev\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_curr, loc = loc, scale = scale)\n",
    "    \n",
    "    def step_sample(self, key, x_prev, y_curr, theta):\n",
    "        \"\"\"\n",
    "        Sample from the proposal distribution\n",
    "        \"\"\"\n",
    "        loc = x_prev\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def prior_lpdf(self, x_init, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the initial state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_init, loc=0.0, scale=scale)\n",
    "\n",
    "    def prior_sample(self, key, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the initial state variable at time `t=0`.\n",
    "        \"\"\"\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return scale * jax.random.normal(key=key)\n",
    "\n",
    "    def state_lpdf(self, x_curr, x_prev, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the current state variable at time `t` given the previous state variable at time `t-1`.\n",
    "        \"\"\"\n",
    "        loc = (1 - theta[0] * self._dt) * x_prev\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return jsp.stats.norm.logpdf(x_curr, loc=loc, scale=scale)\n",
    "\n",
    "    def state_sample(self, key, x_prev, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the current state variable at time `t` given the previous state variable at time `t-1`.\n",
    "        \"\"\"\n",
    "        loc = (1 - theta[0] * self._dt) * x_prev\n",
    "        scale = theta[1] * jnp.sqrt(self._dt)\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def meas_lpdf(self, y_curr, x_curr, theta):\n",
    "        \"\"\"\n",
    "        Calculate the log-density of the current measurement variable at time `t` given the current state variable at time `t`.\n",
    "        \"\"\"\n",
    "        loc = x_curr\n",
    "        scale = theta[2]\n",
    "        return jsp.stats.norm.logpdf(y_curr, loc=loc, scale=scale)\n",
    "\n",
    "    def meas_sample(self, key, x_curr, theta):\n",
    "        \"\"\"\n",
    "        Sample one draw from the current measurement variable at time `t` given the current state variable at time `t`.\n",
    "        \"\"\"\n",
    "        loc = x_curr\n",
    "        scale = theta[2]\n",
    "        return loc + scale * jax.random.normal(key=key)\n",
    "\n",
    "    def exact_lpdf(self, y_meas, theta):\n",
    "        \"\"\"\n",
    "        Calculate exact log-density of measurement variables at times `t=0:T`.\n",
    "        \"\"\"\n",
    "        mu_tilde = theta[0] * self._dt\n",
    "#         sigma2_tilde = theta[1] * theta[1] * self._dt\n",
    "        mu = theta[0]\n",
    "        sigma2 = theta[1] * theta[1]\n",
    "        tau2 = theta[2] * theta[2]\n",
    "        n_obs = y_meas.shape[0]  # number of observations\n",
    "        t_meas = jnp.arange(n_obs)\n",
    "        mu_y = jnp.zeros(n_obs)  # mean of y_meas\n",
    "        # variance of y_meas\n",
    "        Sigma_y = jax.vmap(lambda t: jnp.abs(t - t_meas))(t_meas)\n",
    "        Sigma_y = jnp.exp(- mu_tilde * Sigma_y)\n",
    "#         Sigma_y = sigma2_tilde / (2*mu_tilde) * Sigma_y\n",
    "        Sigma_y = sigma2 / (2*mu) * Sigma_y\n",
    "        Sigma_y = Sigma_y + tau2 * jnp.eye(n_obs)\n",
    "        return jsp.stats.multivariate_normal.logpdf(\n",
    "            x=y_meas, mean=mu_y, cov=Sigma_y\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29caf13a",
   "metadata": {},
   "source": [
    "### Simulate Data\n",
    "\n",
    "This is accomplished with the function `pfjax.simulate()`. The procedure is essentially the same as what we did in the BMModel example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcdd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T15:23:16.361509Z",
     "start_time": "2022-11-15T15:23:16.058854Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameter values\n",
    "mu = 0.4\n",
    "sigma = .2\n",
    "tau = .1\n",
    "theta_true = jnp.array([mu, sigma, tau])\n",
    "\n",
    "# data specification\n",
    "dt = .5\n",
    "n_obs = 100\n",
    "x_init = jnp.array(0.)\n",
    "\n",
    "# initial key for random numbers\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# simulate data\n",
    "ou_model = OUModel(dt=dt)\n",
    "key, subkey = jax.random.split(key)\n",
    "y_meas, x_state = pf.simulate(\n",
    "    model=ou_model,\n",
    "    key=subkey,\n",
    "    n_obs=n_obs,\n",
    "    x_init=x_init,\n",
    "    theta=theta_true\n",
    ")\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"x_state\": jnp.squeeze(x_state),\n",
    "                         \"y_meas\": jnp.squeeze(y_meas)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ee197",
   "metadata": {},
   "source": [
    "### Calculate Particle Filter Marginal Logikelihood\n",
    "\n",
    "We can use the built-in particle filter provided by `pfjax.particle_filter()`.  We'll also compare the particle filter approximation $\\hat{\\ell}(\\tth)$ to the exact loglikelihood $\\ell(\\tth)$ using \"projection plots\", i.e., we'll plot the one-dimensional marginal loglikelihood in each of the parameters $\\tth = (\\mu, \\sigma, \\tau)$ with the other two parameters fixed at their simulated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a0c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T15:23:27.668001Z",
     "start_time": "2022-11-15T15:23:17.883668Z"
    }
   },
   "outputs": [],
   "source": [
    "# particle filter specification\n",
    "n_particles = 200  # number of particles\n",
    "\n",
    "\n",
    "def ou_loglik_pf_nojit(theta, key):\n",
    "    \"\"\"\n",
    "    Particle filter loglikelihood of the OU model (un-jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    subkeys = jax.random.split(key, num=theta.shape[0])\n",
    "    return jax.vmap(lambda _theta, _key: pf.particle_filter(\n",
    "        model=ou_model,\n",
    "        key=_key,\n",
    "        y_meas=y_meas,\n",
    "        n_particles=n_particles,\n",
    "        theta=_theta,\n",
    "        history=False,\n",
    "        score=False,\n",
    "        fisher=False\n",
    "    )[\"loglik\"])(theta, subkeys)\n",
    "\n",
    "\n",
    "# jitted version\n",
    "ou_loglik_pf_jit = jax.jit(ou_loglik_pf_nojit)\n",
    "\n",
    "# check jit speedup\n",
    "key, subkey = jax.random.split(key)\n",
    "%timeit ou_loglik_pf_nojit(theta=theta_true, key=subkey)\n",
    "%timeit ou_loglik_pf_jit(theta=theta_true, key=subkey)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ou_loglik_exact(theta):\n",
    "    \"\"\"\n",
    "    Exact loglikelihood of the OU model (jitted).\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    return jax.vmap(lambda _theta: ou_model.exact_lpdf(\n",
    "        theta=_theta,\n",
    "        y_meas=y_meas\n",
    "    ))(theta)\n",
    "\n",
    "# projection plot specification\n",
    "n_pts = 100  # number of evaluation points per plot\n",
    "# plot limits for each parameter\n",
    "theta_lims = jnp.array([[0, 1.0], [.05, .5], [.05, .2]])\n",
    "theta_names = [\"mu\", \"sigma\", \"tau\"]  # parameter names\n",
    "\n",
    "# calculate projection plot for exact loglikelihood\n",
    "df_exact = pjp.proj_plot(\n",
    "    fun=ou_loglik_exact,\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# calculate projection plot for particle filter loglikelihood\n",
    "df_pf = pjp.proj_plot(\n",
    "    fun=partial(ou_loglik_pf_jit, key=subkey),\n",
    "    x_opt=theta_true,\n",
    "    x_lims=theta_lims,\n",
    "    x_names=theta_names,\n",
    "    n_pts=n_pts,\n",
    "    vectorized=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# merge data frames and plot them\n",
    "plot_df = pd.concat([df_exact, df_pf], ignore_index=True)\n",
    "plot_df[\"type\"] = np.repeat([\"exact\", \"pf\"], len(df_exact[\"variable\"]))\n",
    "plot_df = plot_df.rename(columns={\"y\": \"loglikelihood\"})\n",
    "rp = sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"x\", y=\"loglikelihood\",\n",
    "    hue=\"type\",\n",
    "    col=\"variable\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "rp.set_titles(col_template=\"{col_name}\")\n",
    "rp.set(xlabel=None)\n",
    "# add true parameter values\n",
    "for ax, theta in zip(rp.axes.flat, theta_true):\n",
    "    ax.axvline(theta, linestyle=\"--\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7b91d",
   "metadata": {},
   "source": [
    "<a id=\"bm_deriv\"></a>\n",
    "## Appendix: Exact Likelihood of the BM Model\n",
    "\n",
    "The distribution of $p(\\xx_{0:T}, \\yy_{0:T} \\mid \\tth)$ is multivariate normal.  Thus, $p(\\yy_{0:T} \\mid \\tth)$ is also multivariate normal, and we only need to find $E[y_t \\mid \\tth]$ and $\\cov(y_s, y_t \\mid \\tth)$.\n",
    "\n",
    "Conditioned on $x_0$ and $\\tth$, the Brownian latent variables $\\xx_{1:T}$ are multivariate normal with\n",
    "\n",
    "$$\n",
    "\\newcommand{\\cov}{\\operatorname{cov}}\n",
    "\\begin{aligned}\n",
    "E[x_t \\mid x_0, \\tth] & = x_0 + \\tilde \\mu t, \\\\\n",
    "\\cov(x_s, x_t \\mid x_0, \\tth) & = \\tilde \\sigma^2 \\min(s, t),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde \\mu = \\mu \\dt$ and $\\tilde \\sigma^2 = \\sigma^2 \\dt$.\n",
    "\n",
    "Therefore, $p(\\xx_{0:T} \\mid \\tth)$ is multivariate normal with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[x_t \\mid \\tth] & = E[E[x_t \\mid x_0, \\tth]] \\\\\n",
    "& = \\tilde \\mu t, \\\\\n",
    "\\cov(x_s, x_t \\mid \\tth) & = \\cov(E[x_s \\mid x_0, \\tth], E[x_t \\mid x_0, \\tth]) + E[\\cov(x_s, x_t \\mid x_0, \\tth)] \\\\\n",
    "& = \\tilde \\sigma^2 (1 + \\min(s, t)).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, conditioned on $\\xx_{0:T}$ and $\\tth$, the measurement variables $\\yy_{0:T}$ are multivariate normal with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid \\xx_{0:T}, \\tth] & = x_t, \\\\\n",
    "\\cov(y_s, y_t \\mid \\xx_{0:T}, \\tth) & = \\tau^2 \\delta_{st}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, $p(\\yy_{0:T} \\mid \\tth)$ is multivariate normal with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid \\tth] & = \\tilde \\mu t, \\\\\n",
    "\\cov(y_s, y_t \\mid \\tth) & = \n",
    "% \\cov(E[y_s \\mid \\xx_{0:T}, \\tth], E[y_t \\mid \\xx_{0:T}, \\tth]) + E[\\cov(y_s, y_t \\mid \\xx_{0:T}, \\tth)] \\\\\n",
    "\\tilde \\sigma^2 (1 + \\min(s, t)) + \\tau^2 \\delta_{st}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c3fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds]",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
