{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d416f1",
   "metadata": {},
   "source": [
    "# Comparison of Gradient and Hessian Estimation for Resampling Methods\n",
    "\n",
    "We are interested in comparing: \n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item \\textbf{Bias in gradient estimates}: Determining which method is the least biased for a large number of particles\n",
    "    \\item \\textbf{Relationship between bias and inference time}: Does the bias for each method decrease with the amount of compute time (i.e. number of samples)\n",
    "    \\item \\textbf{Variance in gradient estimates}: Are the signs of the gradient estimates correct for each resampling method. If this is not the case for a particular resampling method, the stochastic optimizer can get stuck or go in the wrong direction\n",
    "\\end{enumerate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import projplot.proj_plot as pjp\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set()\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "\n",
    "import pfjax as pf\n",
    "from pfjax import particle_resamplers as resampler\n",
    "from pfjax.models import BMModel\n",
    "from pfjax.experimental.models import LotVolModelLog\n",
    "import optax\n",
    "from jax import lax\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# again, this only works on startup!\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c82298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfjax.utils import lwgt_to_prob, continuous_cdf, interpolate_weights, argsort_marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998a3ee",
   "metadata": {},
   "source": [
    "## Brownian Motion Model with Drift\n",
    "\n",
    "\\begin{align*}\n",
    "    x_0 &\\sim N(0, \\sigma^2 \\Delta t) \\\\\n",
    "    x_t &\\sim N(x_{t-1} + \\mu \\Delta t, \\sigma^2 \\Delta t) \\\\\n",
    "    y_t &\\sim N(x_t, \\tau^2) \\\\\n",
    "    \\boldsymbol{\\theta} &= (\\mu, \\sigma, \\tau) \n",
    "\\end{align*}\n",
    "\n",
    "The true parameter values used are: \n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\theta} &= (\\mu = 0.25, \\sigma = 0.5, \\tau = 0.2) \\\\\n",
    "\\Delta t &= 0.1 \\\\ \n",
    "n &= 100 \\\\\n",
    "x_0 &= 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter values\n",
    "mu = 0.25\n",
    "sigma = .5\n",
    "tau = .2\n",
    "theta_true_bm = jnp.array([mu, sigma, tau])\n",
    "\n",
    "# data specification\n",
    "dt = .1\n",
    "n_obs = 100\n",
    "x_init = jnp.array(0.)\n",
    "\n",
    "# initial key for random numbers\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# simulate data\n",
    "bm_model = BMModel(dt=dt)\n",
    "key, subkey = jax.random.split(key)\n",
    "y_meas_bm, x_state_bm = pf.simulate(\n",
    "    model=bm_model,\n",
    "    key=subkey,\n",
    "    n_obs=n_obs,\n",
    "    x_init=x_init,\n",
    "    theta=theta_true_bm\n",
    ")\n",
    "\n",
    "# projection plot specification\n",
    "n_pts = 100 # number of evaluation points per plot\n",
    "theta_lims_bm = jnp.array([[-.5, 1], \n",
    "                           [.2, .9], \n",
    "                           [.13, .5]])  # plot limits for each parameter\n",
    "theta_names_bm = [\"mu\", \"sigma\", \"tau\"] # parameter names\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"x_state\": jnp.squeeze(x_state_bm),\n",
    "                         \"y_meas\": jnp.squeeze(y_meas_bm)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "g = sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\",\n",
    "    height=4, aspect=1.5\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb25a9",
   "metadata": {},
   "source": [
    "### Log-likelihood Projection Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada75cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_continuous_bm (key, x_particles_prev, logw):\n",
    "    \"\"\"\n",
    "    Continuous CDF resampler for the Brownian motion model with drift\n",
    "    \"\"\"\n",
    "    p_shape = x_particles_prev.shape\n",
    "    n_particles = p_shape[0]\n",
    "    prob = lwgt_to_prob(logw)\n",
    "\n",
    "    sorted_marginals = argsort_marginal(x_particles_prev, prob)\n",
    "\n",
    "    U = random.uniform(key, shape=(n_particles,))\n",
    "    x_particles = jax.vmap(\n",
    "        lambda u: continuous_cdf(sorted_marginals[\"x\"], sorted_marginals[\"w\"], u), in_axes = (0))(U)\n",
    "    return {\n",
    "        \"x_particles\": x_particles,\n",
    "    }\n",
    "\n",
    "def bm_neg_loglik_exact(theta, y_meas):\n",
    "    \"\"\"\n",
    "    Exact negative loglikelihood of the BM model.\n",
    "    \"\"\"\n",
    "    theta = jnp.atleast_2d(theta)\n",
    "    return -jax.vmap(lambda _theta: bm_model.loglik_exact(\n",
    "        y_meas=y_meas, \n",
    "        theta=_theta\n",
    "    ))(theta)[0]\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(2,3,))\n",
    "def neg_loglik(theta, y_meas, model, key, n_particles=500, resample_fn=resampler.resample_multinomial):\n",
    "    \"\"\" negative log-likelihood of PF with continuous resampling \"\"\"\n",
    "    ret = pf.particle_filter(\n",
    "        theta=theta, \n",
    "        model=model, \n",
    "        y_meas=y_meas, \n",
    "        n_particles=n_particles, \n",
    "        key=key,\n",
    "        resampler = resample_fn,\n",
    "        history = False)\n",
    "    return -ret[\"loglik\"]\n",
    "\n",
    "\n",
    "# set the max_iterations to 10 to reduce runtimes\n",
    "pointcloud_kwargs = {\"epsilon\": 0.75}\n",
    "sinkhorn_kwargs = {\"max_iterations\": 100}\n",
    "def resample_ot_jit(key, x_particles_prev, logw):\n",
    "    return resampler.resample_ot(key, x_particles_prev, logw,\n",
    "                                 pointcloud_kwargs = pointcloud_kwargs,\n",
    "                                 sinkhorn_kwargs = sinkhorn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5916c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik_compare (estimators, estimator_names, theta_true, theta_lims, theta_names, n_pts=100):\n",
    "    \"\"\"\n",
    "    Return plot/df to compare log-likelihood estimators\n",
    "    \n",
    "    Args: \n",
    "        - estimators: list of estimators \n",
    "        - estimator_names: name of estimators used when combining the projection plots\n",
    "        - plot: boolean whether or not to plot the projection plots\n",
    "        - args: theta_true, theta_lims, theta_names, n_pts\n",
    "    Returns\n",
    "    \"\"\"\n",
    "    assert len(estimator_names) == len(estimators)\n",
    "    df_list = []\n",
    "    for i, est in enumerate(estimators):\n",
    "        _proj_plot = pjp.proj_plot(\n",
    "            fun=est,\n",
    "            x_opt=theta_true, \n",
    "            x_lims=theta_lims, \n",
    "            x_names=theta_names, \n",
    "            n_pts=n_pts,\n",
    "            vectorized=False,\n",
    "            plot=False\n",
    "        )\n",
    "        df_list.append(_proj_plot)\n",
    "        \n",
    "    plot_df = pd.concat(df_list, ignore_index=True)\n",
    "    plot_df[\"Resampler\"] = np.repeat(estimator_names, _proj_plot.shape[0])\n",
    "    plot_df = plot_df.rename(columns={\"y\": \"neg-loglikelihood\"})\n",
    "    return plot_df\n",
    "\n",
    "\n",
    "def approx_score_fn (loglik, theta_true, theta_lims, theta_names, n_pts=100):\n",
    "    \"\"\"\n",
    "    Approximate the profile log-likelihood, `loglik` with a cubic regression \n",
    "    \n",
    "    Args: \n",
    "        - loglik: negative loglikelihood function to be passed to pjp.proj_plot, function accepting theta only\n",
    "        - theta_true: True value of theta\n",
    "        - theta_lims: limits for each value of theta for which to calculate the profile likelihood\n",
    "        - theta_names: names\n",
    "        - grad_fn: Default None, if specified this is the gradient of the log-likelihood, which is evaluated over the grid (theta_lims)\n",
    "        \n",
    "    Returns: \n",
    "        - Dataframe of log-likelihood approximation \n",
    "        - Plot of log-likelihood and its approximation \n",
    "        - Values of the (approximate) score funciton evaluated at the true parameter values\n",
    "    \"\"\"\n",
    "    \n",
    "    ## estimate of log-likelihood:\n",
    "    mc_loglik_df = pjp.proj_plot(\n",
    "        fun=loglik,\n",
    "        x_opt=theta_true, \n",
    "        x_lims=theta_lims, \n",
    "        x_names=theta_names, \n",
    "        n_pts=n_pts,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    # log-likelihood approximation with regression: \n",
    "    data = mc_loglik_df.copy()\n",
    "    data[\"y_hat\"] = np.ones(data.shape[0])\n",
    "    data[\"loglik_grad\"] = np.ones(data.shape[0])\n",
    "\n",
    "    true_grad = np.zeros(len(theta_true))\n",
    "    for i, _param in enumerate(mc_loglik_df.x_opt.unique()):\n",
    "        tmp_data = mc_loglik_df[mc_loglik_df.x_opt == _param].reset_index(drop = True)\n",
    "\n",
    "        # fit quadratic regression: y = x^3 + x^2 + x + c\n",
    "        X = pd.DataFrame(np.stack([np.ones(tmp_data.shape[0]), \n",
    "                                   tmp_data[\"x\"], \n",
    "                                   tmp_data[\"x\"]**2,\n",
    "                                   tmp_data[\"x\"]**3], axis = 1).astype('float64'), \n",
    "                         columns=[\"intercept\", \"x\", \"x^2\", \"x^3\"]).reset_index(drop = True)\n",
    "        beta_hat = np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, tmp_data[\"y\"]))\n",
    "        # cast to array to avoid index issues\n",
    "        data.loc[data.x_opt == _param, \"y_hat\"] = np.array(np.matmul(X, beta_hat))\n",
    "\n",
    "        # estimate grad on grid (theta_lims):\n",
    "        loglik_grad = np.array(beta_hat[1] + 2*beta_hat[2]*tmp_data[\"x\"] + 3*beta_hat[3]*tmp_data[\"x\"]**2)\n",
    "        data.loc[data.x_opt == _param, \"loglik_grad\"] = loglik_grad\n",
    "        \n",
    "        # score function evaluated at the true parameters: (beta_1 + 2*beta_2 + 3*beta_3^2)\n",
    "        true_grad[i] = np.array(beta_hat[1] + 2*beta_hat[2]*theta_true[i] + 3*beta_hat[3]*theta_true[i]**2)\n",
    "    \n",
    "    return true_grad, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection plots for multinomial\n",
    "n_particles_multinom = 50\n",
    "n_particles_ot = 50\n",
    "n_particles_mvn = 50\n",
    "n_particles_gc = 50\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "# multinom resampling\n",
    "bm_multinom = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_multinom,\n",
    "    resample_fn = resampler.resample_multinomial))\n",
    "\n",
    "# continuous CDF resampling\n",
    "bm_cont = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_gc,\n",
    "    resample_fn = resample_continuous_bm))\n",
    "\n",
    "# MVN resampling\n",
    "bm_mvn = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_mvn,\n",
    "    resample_fn = resampler.resample_mvn))\n",
    "\n",
    "# OTT resampling\n",
    "bm_ott = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_ot,\n",
    "    resample_fn = resample_ot_jit))\n",
    "\n",
    "%timeit bm_multinom(theta_true_bm)\n",
    "%timeit bm_cont(theta_true_bm)\n",
    "%timeit bm_mvn(theta_true_bm)\n",
    "%timeit bm_ott(theta_true_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db85019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create projection plots for each log-likelihood estimator:\n",
    "plot_args = {\n",
    "    \"theta_true\": theta_true_bm,\n",
    "    \"theta_lims\": theta_lims_bm,\n",
    "    \"theta_names\": theta_names_bm,\n",
    "    \"n_pts\": n_pts\n",
    "}\n",
    "\n",
    "# plot_df = loglik_compare(\n",
    "#     [bm_multinom, bm_mvn, bm_cont, bm_ott],\n",
    "#     [\"Multinomial\", \"MVN\", \"Continuous\", \"Optimal Transport\"],\n",
    "#     **plot_args\n",
    "# )\n",
    "\n",
    "plot_df = loglik_compare(\n",
    "    [bm_multinom, bm_mvn, bm_cont],\n",
    "    [\"Multinomial\", \"MVN\", \"Continuous\"],\n",
    "    **plot_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee990399",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.relplot(\n",
    "    data=plot_df,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-loglikelihood\", \n",
    "    hue=\"Resampler\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 2,\n",
    "    legend = \"full\",\n",
    "    height=4, #aspect=1.3,\n",
    "    facet_kws=dict(sharex=False, sharey=True)\n",
    ")\n",
    "\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None);\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta_true_bm[ind], color='red', lw = 0.9, label=\"True Parameter Value\")\n",
    "\n",
    "_ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d27909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max_iterations to 10 to reduce runtimes\n",
    "pointcloud_kwargs = {\"epsilon\": 0.25}\n",
    "sinkhorn_kwargs = {\"max_iterations\": 400}\n",
    "def resample_ot_jit(key, x_particles_prev, logw):\n",
    "    return resampler.resample_ot(key, x_particles_prev, logw,\n",
    "                                 pointcloud_kwargs = pointcloud_kwargs,\n",
    "                                 sinkhorn_kwargs = sinkhorn_kwargs)\n",
    "\n",
    "# OTT resampling\n",
    "bm_ott = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_ot,\n",
    "    resample_fn = resample_ot_jit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26536090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create projection plots for each log-likelihood estimator:\n",
    "ott_df = loglik_compare(\n",
    "    [bm_ott],\n",
    "    [\"Optimal Transport\"],\n",
    "    **plot_args\n",
    ")\n",
    "\n",
    "grid = sns.relplot(\n",
    "    data=ott_df,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-loglikelihood\", \n",
    "    hue=\"Resampler\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None);\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta_true_bm[ind], color='red', lw = 0.9, label=\"True\")\n",
    "_ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afa7f8",
   "metadata": {},
   "source": [
    "### Check Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # score function for different resampling schemes: \n",
    "bm_loglik = partial(neg_loglik, model=bm_model, y_meas = y_meas_bm)\n",
    "bm_score_exact = jax.jit(jax.grad(partial(bm_neg_loglik_exact, y_meas=y_meas_bm)))\n",
    "\n",
    "# multinomial resampling:\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_score_multinom(theta, key, n_particles):\n",
    "    return jax.grad(bm_loglik)(theta, key=key, n_particles=n_particles)\n",
    "\n",
    "# MVN resampling: \n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_score_mvn(theta, key, n_particles):\n",
    "    return jax.grad(bm_loglik)(theta, key=key, n_particles=n_particles, \n",
    "                               resample_fn = pf.particle_resamplers.resample_mvn)\n",
    "\n",
    "# continuous CDF resampling: \n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_score_cont(theta, key, n_particles):\n",
    "    return jax.grad(bm_loglik)(theta, key=key, n_particles=n_particles, \n",
    "                               resample_fn = resample_continuous_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da30f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy of our approximation of the true score function: \n",
    "key, subkey = random.split(key)\n",
    "_bm_multinom = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=10_000))\n",
    "\n",
    "bm_score_test, _ = approx_score_fn(\n",
    "    _bm_multinom, theta_true_bm, theta_lims_bm, theta_names_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot true log-likelihood and its regression approximation\n",
    "plot_data = pd.melt(_, id_vars=[\"x\", \"x_opt\", \"loglik_grad\"], value_name=\"neg-ll\", var_name=\"method\")\n",
    "\n",
    "# rename for nicer plot: \n",
    "plot_data.loc[plot_data.method == \"y\", \"method\"] = \"True Log-lik\"\n",
    "plot_data.loc[plot_data.method == \"y_hat\", \"method\"] = \"Approx. Log-lik\"\n",
    "\n",
    "grid = sns.relplot(\n",
    "    data = plot_data,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-ll\", \n",
    "    hue = \"method\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 4,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None)\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta_true_bm[ind], color='red', lw = 0.9, label=\"True\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d429a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "n_particles = 100\n",
    "n_sim = 100\n",
    "key, *subkeys = random.split(key, n_sim+1)\n",
    "\n",
    "# repeat calculation nsim times\n",
    "score_exact = bm_score_exact(theta_true_bm)\n",
    "score_multinom = []\n",
    "score_mvn = []\n",
    "score_cont = []\n",
    "\n",
    "for i in range(n_sim):\n",
    "    score_multinom += [bm_score_multinom(theta_true_bm, subkeys[i], n_particles=n_particles)]\n",
    "    score_mvn += [bm_score_mvn(theta_true_bm, subkeys[i], n_particles=n_particles)]\n",
    "    score_cont += [bm_score_cont(theta_true_bm, subkeys[i], n_particles=n_particles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = (\n",
    "    pd.DataFrame({\n",
    "    \"theta\": np.tile(theta_names_bm, n_sim),\n",
    "    \"multinom\": np.array(score_multinom).ravel(),\n",
    "    \"mvn\": np.array(score_mvn).ravel(),\n",
    "    \"cont\": np.array(score_cont).ravel()\n",
    "})\n",
    "    .melt(id_vars=[\"theta\"], value_vars=[\"multinom\", \"mvn\", \"cont\"], var_name=\"method\")\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=score_df,\n",
    "    kind=\"box\",\n",
    "    x=\"method\", y=\"value\",\n",
    "    col=\"theta\",\n",
    "    col_wrap=3,\n",
    "    sharey=False\n",
    ")\n",
    "[g.axes[i].axhline(score_exact[i]) for i in range(theta_true_bm.size)];\n",
    "# [g.axes[i].axhline(bm_score_test[i], color = \"firebrick\") for i in range(bm_score_test.size)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "n_particles = 2500\n",
    "n_sim = 100\n",
    "key, *subkeys = random.split(key, n_sim+1)\n",
    "\n",
    "# repeat calculation nsim times\n",
    "score_exact = bm_score_exact(theta_true_bm)\n",
    "score_multinom = []\n",
    "score_mvn = []\n",
    "score_cont = []\n",
    "\n",
    "for i in range(n_sim):\n",
    "    score_multinom += [bm_score_multinom(theta_true_bm, subkeys[i], n_particles=n_particles)]\n",
    "    score_mvn += [bm_score_mvn(theta_true_bm, subkeys[i], n_particles=n_particles)]\n",
    "    score_cont += [bm_score_cont(theta_true_bm, subkeys[i], n_particles=n_particles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8977e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = (\n",
    "    pd.DataFrame({\n",
    "    \"theta\": np.tile(theta_names_bm, n_sim),\n",
    "    \"multinom\": np.array(score_multinom).ravel(),\n",
    "    \"mvn\": np.array(score_mvn).ravel(),\n",
    "    \"cont\": np.array(score_cont).ravel()\n",
    "})\n",
    "    .melt(id_vars=[\"theta\"], value_vars=[\"multinom\", \"mvn\", \"cont\"], var_name=\"method\")\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=score_df,\n",
    "    kind=\"box\",\n",
    "    x=\"method\", y=\"value\",\n",
    "    col=\"theta\",\n",
    "    col_wrap=3,\n",
    "    sharey=False\n",
    ")\n",
    "[g.axes[i].axhline(score_exact[i]) for i in range(theta_true_bm.size)];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97873d61",
   "metadata": {},
   "source": [
    "### Check Fisher Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # score function for different resampling schemes: \n",
    "bm_fisher_exact = jax.jit(jax.jacfwd(jax.jacrev(partial(bm_neg_loglik_exact, y_meas=y_meas_bm))))\n",
    "\n",
    "# multinomial resampling:\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_fisher_multinom(theta, key, n_particles):\n",
    "    return jax.jacfwd(jax.jacrev(bm_loglik))(theta, \n",
    "                                             key=key, n_particles=n_particles)\n",
    "\n",
    "# MVN resampling: \n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_fisher_mvn(theta, key, n_particles):\n",
    "    return jax.jacfwd(jax.jacrev(bm_loglik))(theta, key=key, n_particles=n_particles, \n",
    "                                             resample_fn = pf.particle_resamplers.resample_mvn)\n",
    "\n",
    "# continuous CDF resampling: \n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_fisher_cont(theta, key, n_particles):\n",
    "    return jax.jacfwd(jax.jacrev(bm_loglik))(theta, key=key, n_particles=n_particles, \n",
    "                                             resample_fn = resample_continuous_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "n_particles = 100\n",
    "n_sim = 100\n",
    "key, *subkeys = random.split(key, n_sim+1)\n",
    "\n",
    "# repeat calculation nsim times\n",
    "fisher_exact = bm_fisher_exact(theta_true_bm)\n",
    "fisher_multinom = []\n",
    "fisher_mvn = []\n",
    "fisher_cont = []\n",
    "\n",
    "for i in range(n_sim):\n",
    "    fisher_multinom += [bm_fisher_multinom(theta_true_bm, subkeys[i], n_particles)]\n",
    "    fisher_mvn += [bm_fisher_mvn(theta_true_bm, subkeys[i], n_particles)]\n",
    "    fisher_cont += [bm_fisher_cont(theta_true_bm, subkeys[i], n_particles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_theta_names_bm = np.meshgrid(np.array(theta_names_bm), np.array(theta_names_bm))\n",
    "fisher_theta_names_bm = np.array(\n",
    "    [fisher_theta_names_bm[1].ravel()[i] + '_' +\n",
    "     fisher_theta_names_bm[0].ravel()[i]\n",
    "     for i in range(fisher_theta_names_bm[0].size)]\n",
    ")\n",
    "\n",
    "plot_df = (\n",
    "    pd.DataFrame({\n",
    "    \"theta\": np.tile(fisher_theta_names_bm, n_sim),\n",
    "    \"multinomial\": np.array(fisher_multinom).ravel(),\n",
    "    \"MVN\": np.array(fisher_mvn).ravel(),\n",
    "    \"Continuous\": np.array(fisher_cont).ravel()\n",
    "})\n",
    "    .melt(id_vars=[\"theta\"], value_vars=[\"multinomial\", \"MVN\", \"Continuous\"], var_name=\"method\")\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_df, kind=\"box\",\n",
    "    x=\"method\", y=\"value\",\n",
    "    col=\"theta\",\n",
    "    col_wrap=3,\n",
    "    sharey=False\n",
    ")\n",
    "[g.axes[i].axhline(fisher_exact.ravel()[i]) for i in range(fisher_theta_names_bm.size)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "n_particles = 2500\n",
    "n_sim = 100\n",
    "key, *subkeys = random.split(key, n_sim+1)\n",
    "\n",
    "# repeat calculation nsim times\n",
    "fisher_exact = bm_fisher_exact(theta_true_bm)\n",
    "fisher_multinom = []\n",
    "fisher_mvn = []\n",
    "fisher_cont = []\n",
    "\n",
    "for i in range(n_sim):\n",
    "    fisher_multinom += [bm_fisher_multinom(theta_true_bm, subkeys[i], n_particles)]\n",
    "    fisher_mvn += [bm_fisher_mvn(theta_true_bm, subkeys[i], n_particles)]\n",
    "    fisher_cont += [bm_fisher_cont(theta_true_bm, subkeys[i], n_particles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2dd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_theta_names_bm = np.meshgrid(np.array(theta_names_bm), np.array(theta_names_bm))\n",
    "fisher_theta_names_bm = np.array(\n",
    "    [fisher_theta_names_bm[1].ravel()[i] + '_' +\n",
    "     fisher_theta_names_bm[0].ravel()[i]\n",
    "     for i in range(fisher_theta_names_bm[0].size)]\n",
    ")\n",
    "\n",
    "plot_df = (\n",
    "    pd.DataFrame({\n",
    "    \"theta\": np.tile(fisher_theta_names_bm, n_sim),\n",
    "    \"multinomial\": np.array(fisher_multinom).ravel(),\n",
    "    \"MVN\": np.array(fisher_mvn).ravel(),\n",
    "    \"Continuous\": np.array(fisher_cont).ravel()\n",
    "})\n",
    "    .melt(id_vars=[\"theta\"], value_vars=[\"multinomial\", \"MVN\", \"Continuous\"], var_name=\"method\")\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_df, kind=\"box\",\n",
    "    x=\"method\", y=\"value\",\n",
    "    col=\"theta\",\n",
    "    col_wrap=3,\n",
    "    sharey=False\n",
    ")\n",
    "[g.axes[i].axhline(fisher_exact.ravel()[i]) for i in range(fisher_theta_names_bm.size)];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50933e7e",
   "metadata": {},
   "source": [
    "### Brownian Motion Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_timing(loglik_list, theta, n_particles, n_sim=100):\n",
    "    \"\"\"\n",
    "    Function to time particle filters\n",
    "    \n",
    "    Returns average time to run each model in loglik_list with n_particles\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    key = random.PRNGKey(0)\n",
    "    key, *subkeys = random.split(key, n_sim+1)\n",
    "    for ll in loglik_list: \n",
    "        start = time.perf_counter()\n",
    "        _ = [ll(theta, subkeys[i], n_particles=n_particles) for i in range(n_sim)]\n",
    "        times.append((time.perf_counter() - start)/n_sim)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ac12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_score_time = lambda x: pf_timing(\n",
    "    loglik_list = [bm_score_multinom, bm_score_mvn, bm_score_cont], \n",
    "    theta = theta_true_bm, \n",
    "    n_particles = x)\n",
    "\n",
    "n_particles_list = [50, 250, 500, 1000]\n",
    "bm_score_timings = list(map(bm_score_time, n_particles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_particles_list, bm_score_timings)\n",
    "plt.title(\"Inference time VS # of particle for BM Model\")\n",
    "plt.xlabel(\"Number of Particles\")\n",
    "plt.ylabel(\"Inference time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bce2f",
   "metadata": {},
   "source": [
    "#### Optimal Transport - BM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import ott\n",
    "from ott.geometry import costs, pointcloud\n",
    "from ott.core import sinkhorn\n",
    "from ott.core import linear_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_points(rng, n, m, d):\n",
    "    rngs = jax.random.split(rng, 3)\n",
    "    x = jax.random.normal(rngs[0], (n, d)) + 1\n",
    "    y = jax.random.uniform(rngs[1], (m, d))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "n, m, d = 13, 17, 2\n",
    "x, y = create_points(rng, n=n, m=m, d=d)\n",
    "geom = pointcloud.PointCloud(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear problem with that cost structure.\n",
    "ot_prob = linear_problems.LinearProblem(geom)\n",
    "\n",
    "a = jnp.ones(len(x)) / len(x)\n",
    "\n",
    "_b = np.random.uniform(size = len(x))\n",
    "b = _b / sum(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sinkhorn solver\n",
    "# solver = sinkhorn.Sinkhorn()\n",
    "# ot = solver(ot_prob)\n",
    "ot = sinkhorn.sinkhorn(geom, a=a, b=b, threshold = 1e-10)\n",
    "\n",
    "# The out object contains many things, among which the regularized OT cost\n",
    "print(\n",
    "    \" Sinkhorn has converged: \",\n",
    "    ot.converged,\n",
    "    \"\\n\",\n",
    "    \"Error upon last iteration: \",\n",
    "    ot.errors[(ot.errors > -1)][-1],\n",
    "    \"\\n\",\n",
    "    \"Sinkhorn required \",\n",
    "    jnp.sum(ot.errors > -1),\n",
    "    \" iterations to converge. \\n\",\n",
    "    \"Entropy regularized OT cost: \",\n",
    "    ot.reg_ot_cost,\n",
    "    \"\\n\",\n",
    "    \"OT cost (without entropy): \",\n",
    "    jnp.sum(ot.matrix * ot.geom.cost_matrix),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac63b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_opt(f, g, x, w, eps=1):\n",
    "    \"\"\"\n",
    "    Manually apply transport matrix using potentials f, g\n",
    "    \"\"\"\n",
    "    def func (w, f, g, xi, xj):\n",
    "        return w * jnp.exp((f + g - jnp.linalg.norm(xi - xj, ord=2))/eps)\n",
    "                           \n",
    "    num_particles = x.shape[0]\n",
    "                           \n",
    "    trans = jax.vmap(\n",
    "        lambda i: jax.vmap(lambda j: func(w[i], f[i], g[j], x[i], x[j])/num_particles)(jnp.arange(num_particles))\n",
    "    )(jnp.arange(num_particles))\n",
    "                           \n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a24460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P^{OT} JAX way: \n",
    "jax_trans = p_opt(\n",
    "    f=jnp.array(ot.f), g=jnp.array(ot.g), \n",
    "    x=jnp.array(x), w=jnp.array(b))\n",
    "\n",
    "# P^{OT} numpy way: \n",
    "np_trans = np.zeros((len(x), len(x)))\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x)):\n",
    "        np_trans[i,j] = b[i] * jnp.exp(ot.f[i] + ot.g[j] - jnp.linalg.norm(x[i] - x[j], ord=2)) / len(x)\n",
    "\n",
    "trans - jax_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x) * jnp.matmul(np_trans, x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebe2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the difference between sink.apply() and this matrix multiplacation thing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plott = ott.tools.plot.Plot()\n",
    "_ = plott(ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77594ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ott\n",
    "from ott.geometry import pointcloud\n",
    "from ott.core import sinkhorn\n",
    "\n",
    "def diameter(x, y):\n",
    "    diameter_x = jnp.max(jnp.std(x, 1), -1)\n",
    "    diameter_y = jnp.max(jnp.std(y, 1), -1)\n",
    "    res = jnp.maximum(diameter_x, diameter_y)\n",
    "    return jnp.where(res == 0., 1., res)\n",
    "\n",
    "def scale_x (x):\n",
    "    \"\"\"\n",
    "    Scale X as done in http://proceedings.mlr.press/v139/corenflos21a/corenflos21a.pdf.\n",
    "    \n",
    "    This replicated their code here: \n",
    "    https://github.com/JTT94/filterflow/blob/master/filterflow/resampling/differentiable/regularized_transport/plan.py#L67\n",
    "    \"\"\"\n",
    "    centered_x = x - jnp.mean(x, axis=1, keepdims=True)\n",
    "    diameter_value = diameter(x, x)\n",
    "    scale = jnp.reshape(diameter_value, [-1, 1, 1]) * jnp.sqrt(x.shape[1])\n",
    "    scaled_x = centered_x / scale\n",
    "    return scaled_x.reshape(x.shape)\n",
    "\n",
    "\n",
    "def p_opt(f, g, x, w, eps):\n",
    "    \"\"\"\n",
    "    Manually apply transport matrix using potentials f, g. \n",
    "    \n",
    "    This is p^{OT} in Algorithm 3 in http://proceedings.mlr.press/v139/corenflos21a/corenflos21a.pdf\n",
    "    \"\"\"\n",
    "    def func (w, f, g, xi, xj):\n",
    "        return w * jnp.exp((f + g - sum((xi - xj)**2))/eps)\n",
    "                           \n",
    "    num_particles = x.shape[0]\n",
    "                           \n",
    "    trans = jax.vmap(\n",
    "        lambda i: jax.vmap(lambda j: func(w[i], f[i], g[j], x[i], x[j]))(jnp.arange(num_particles))\n",
    "    )(jnp.arange(num_particles))\n",
    "                           \n",
    "    return trans/num_particles\n",
    "\n",
    "\n",
    "def resample_ot_test(key, x_particles_prev, logw,\n",
    "                pointcloud_kwargs={},\n",
    "                sinkhorn_kwargs={}):\n",
    "    \n",
    "    sinkhorn_kwargs.update(jit=False)\n",
    "    prob = lwgt_to_prob(logw)\n",
    "    p_shape = x_particles_prev.shape\n",
    "    n_particles = p_shape[0]\n",
    "    x_particles = x_particles_prev.reshape((n_particles, -1))\n",
    "    scaled_particles = scale_x(x_particles)\n",
    "    \n",
    "    geom = pointcloud.PointCloud(x=scaled_particles, \n",
    "                                 y=scaled_particles,\n",
    "                                 **pointcloud_kwargs)\n",
    "    sink = sinkhorn.sinkhorn(geom,\n",
    "                             a=jnp.ones(n_particles)/n_particles,\n",
    "                             b=prob,\n",
    "                             **sinkhorn_kwargs)\n",
    "    x_particles_new = sink.apply(x_particles.T)\n",
    "    \n",
    "#     # Manually apply transport: \n",
    "#     opt_trans = p_opt(f=sink.f, g=sink.g, x=x_particles, w=prob, eps=pointcloud_kwargs[\"epsilon\"])\n",
    "#     x_particles_new = n_particles * jnp.matmul(opt_trans, x_particles).T\n",
    "    \n",
    "    return {\n",
    "        \"x_particles\": jnp.reshape(x_particles_new.T, newshape=p_shape),\n",
    "        \"geom\": geom,\n",
    "        \"sink\": sink\n",
    "    }\n",
    "\n",
    "# set the max_iterations to 10 to reduce runtimes\n",
    "pointcloud_kwargs = {\"epsilon\": 0.25}\n",
    "sinkhorn_kwargs = {\"min_iterations\": 0,\n",
    "                   \"threshold\": 1e-10,\n",
    "                   \"inner_iterations\":1,\n",
    "                   \"max_iterations\":1000}\n",
    "n_particles_ot = 100\n",
    "\n",
    "def resample_ot_jit(key, x_particles_prev, logw):\n",
    "    return resample_ot_test(key, x_particles_prev, logw,\n",
    "                            pointcloud_kwargs = pointcloud_kwargs,\n",
    "                            sinkhorn_kwargs = sinkhorn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3d99d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = pf.particle_filter(\n",
    "    theta=theta_true_bm, \n",
    "    model=bm_model, \n",
    "    y_meas=y_meas_bm, \n",
    "    n_particles=n_particles_ot, \n",
    "    key=key,\n",
    "    resampler = resample_ot_jit,\n",
    "    history = True)\n",
    "\n",
    "tmp_true = pf.particle_filter(\n",
    "    theta=theta_true_bm, \n",
    "    model=bm_model, \n",
    "    y_meas=y_meas_bm, \n",
    "    n_particles=n_particles_ot, \n",
    "    key=key,\n",
    "    history = True)\n",
    "\n",
    "tmp_mvn = pf.particle_filter(\n",
    "    theta=theta_true_bm, \n",
    "    model=bm_model, \n",
    "    y_meas=y_meas_bm, \n",
    "    n_particles=n_particles_ot, \n",
    "    key=key,\n",
    "    resampler = resampler.resample_mvn,\n",
    "    history = True)\n",
    "\n",
    "print(tmp[\"loglik\"])\n",
    "print(tmp_true[\"loglik\"])\n",
    "print(tmp_mvn[\"loglik\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converged: \", tmp[\"resample_out\"][\"sink\"].converged)\n",
    "jnp.sum(tmp[\"resample_out\"][\"sink\"].errors > -1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"resample_out\"][\"sink\"].errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90380e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20\n",
    "p1 = jax.random.choice(\n",
    "    key=subkey,\n",
    "    a=tmp[\"x_particles\"][i, :],\n",
    "    shape=(10_00,), \n",
    "    replace=True,\n",
    "    p=pf.utils.lwgt_to_prob(tmp[\"logw\"][i, :]),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "p2 = jax.random.choice(\n",
    "    key=subkey,\n",
    "    a=tmp_true[\"x_particles\"][i, :],\n",
    "    shape=(10_00,), \n",
    "    replace=True,\n",
    "    p=pf.utils.lwgt_to_prob(tmp_true[\"logw\"][i, :]),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "p3 = jax.random.choice(\n",
    "    key=subkey,\n",
    "    a=tmp_mvn[\"x_particles\"][i, :],\n",
    "    shape=(10_00,), \n",
    "    replace=True,\n",
    "    p=pf.utils.lwgt_to_prob(tmp_mvn[\"logw\"][i, :]),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "sns.histplot(p1, label = \"Sink\")\n",
    "sns.histplot(p2, label=\"Multinom\")\n",
    "sns.histplot(p3, label=\"MVN\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTT resampling\n",
    "bm_ott = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_bm,\n",
    "    model = bm_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_ot,\n",
    "    resample_fn = resample_ot_jit))\n",
    "\n",
    "theta_lims_bm = jnp.array([[-0.5, 2], \n",
    "                           [.2, 1.3], \n",
    "                           [.1, .75]])  # plot limits for each parameter\n",
    "\n",
    "plot_args_bm = {\n",
    "    \"theta_true\": theta_true_bm,\n",
    "    \"theta_lims\": theta_lims_bm,\n",
    "    \"theta_names\": theta_names_bm,\n",
    "    \"n_pts\": 50\n",
    "}\n",
    "\n",
    "# function to create projection plots for each log-likelihood estimator:\n",
    "# ott_df = loglik_compare(\n",
    "#     [bm_ott],\n",
    "#     [\"Optimal Transport\"],\n",
    "#     **plot_args_bm\n",
    "# )\n",
    "\n",
    "plot_df = loglik_compare(\n",
    "    [bm_multinom, bm_mvn, bm_cont, bm_ott],\n",
    "    [\"Multinomial\", \"MVN\", \"Continuous\", \"Optimal Transport\"],\n",
    "    **plot_args_bm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08835cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.relplot(\n",
    "    data=plot_df,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-loglikelihood\", \n",
    "    hue=\"Resampler\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None);\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta_true_bm[ind], color='red', lw = 0.9, label=\"True\")\n",
    "_ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532e2ce",
   "metadata": {},
   "source": [
    "## Lotka-Volterra Model\n",
    "\n",
    "\n",
    "- **write model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69164887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter values\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "gamma = 4.0\n",
    "delta = 1.0\n",
    "sigma_h = 0.001\n",
    "sigma_l = 0.001\n",
    "tau_h = 0.25\n",
    "tau_l = 0.25\n",
    "\n",
    "theta_true_lv = np.array([alpha, beta, gamma, delta, sigma_h, sigma_l, tau_h, tau_l])\n",
    "theta_names_lv = [\"alpha\", \"beta\", \"gamma\", \"delta\",\n",
    "               \"sigma_h\", \"sigma_l\", \"tau_h\", \"tau_l\"]\n",
    "theta_lims_lv = np.array(list(zip(theta_true_lv - (theta_true_lv/2), theta_true_lv + (theta_true_lv/2))))\n",
    "\n",
    "dt = 0.1\n",
    "n_res = 1\n",
    "n_obs = 100\n",
    "\n",
    "_key = random.PRNGKey(0)\n",
    "_key, _subkey = random.split(_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113612eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotvol_model = LotVolModelLog(dt, n_res, bootstrap = True) \n",
    "theta_true_lv = jnp.log(theta_true_lv)\n",
    "\n",
    "x_init = jnp.block([[jnp.zeros((n_res-1, 2))],\n",
    "                    [jnp.log(jnp.array([5., 3.]))]])\n",
    "\n",
    "y_meas_lv, x_state_lv = pf.simulate(\n",
    "    model = lotvol_model, \n",
    "    n_obs = n_obs, \n",
    "    x_init = x_init, \n",
    "    theta = theta_true_lv, \n",
    "    key = _subkey)\n",
    "\n",
    "# plt.plot(jnp.exp(y_meas_lv.squeeze()[:, 0]), label = \"Prey - observed\")\n",
    "# plt.plot(jnp.exp(y_meas_lv.squeeze()[:, 1]), label = \"Predator - observed\")\n",
    "# plt.plot(jnp.exp(x_state_lv.squeeze()[:, 0]), label = \"Prey - latent\")\n",
    "# plt.plot(jnp.exp(x_state_lv.squeeze()[:, 1]), label = \"Predator - latent\")\n",
    "\n",
    "plt.plot(y_meas_lv.squeeze()[:, 0], label = \"Prey - observed\")\n",
    "plt.plot(y_meas_lv.squeeze()[:, 1], label = \"Predator - observed\")\n",
    "plt.plot(x_state_lv.squeeze()[:, 0], label = \"Prey - latent\")\n",
    "plt.plot(x_state_lv.squeeze()[:, 1], label = \"Predator - latent\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lims_lv = jnp.log(jnp.array([\n",
    "    [0.8, 1.2],\n",
    "    [0.9, 1.1],\n",
    "    [3.8, 4.2],\n",
    "    [0.95, 1.05],\n",
    "    [0.05, 0.2],\n",
    "    [0.01, 0.2],\n",
    "    [0.1, 0.4],\n",
    "    [0.1, 0.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255698c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# projection plots for multinomial\n",
    "n_particles = 200\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "# multinom resampling\n",
    "lv_multinom = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_lv,\n",
    "    model = lotvol_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles,\n",
    "    resample_fn = resampler.resample_multinomial))\n",
    "\n",
    "# continuous CDF resampling\n",
    "lv_gc = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_lv,\n",
    "    model = lotvol_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles,\n",
    "    resample_fn = resampler.resample_gaussian_copula))\n",
    "\n",
    "# MVN resampling\n",
    "lv_mvn = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_lv,\n",
    "    model = lotvol_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles,\n",
    "    resample_fn = resampler.resample_mvn))\n",
    "\n",
    "\n",
    "%timeit lv_multinom(theta_true_lv)\n",
    "%timeit lv_gc(theta_true_lv)\n",
    "%timeit lv_mvn(theta_true_lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create projection plots for each log-likelihood estimator:\n",
    "plot_args_lv = {\n",
    "    \"theta_true\": theta_true_lv,\n",
    "    \"theta_lims\": theta_lims_lv,\n",
    "    \"theta_names\": theta_names_lv,\n",
    "    \"n_pts\": 100\n",
    "}\n",
    "\n",
    "plot_df_lv = loglik_compare(\n",
    "    [lv_multinom, lv_mvn, lv_gc],\n",
    "    [\"Multinomial\", \"MVN\", \"Gaussian Copula\"], \n",
    "    **plot_args_lv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.relplot(\n",
    "    data=plot_df_lv,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-loglikelihood\", \n",
    "    hue=\"Resampler\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 3,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None);\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(jnp.exp(theta_true_lv[ind]), color='red', lw = 0.9, label=\"True\")\n",
    "\n",
    "_ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a063b",
   "metadata": {},
   "source": [
    "#### Optimal Transport Log-Likelihood\n",
    "\n",
    "Settings from paper: \n",
    "\n",
    "$$\n",
    "\\epsilon = 0.25, 0.5, 0.75 \\\\\n",
    "\\text{n_particles} = 25\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max_iterations to 10 to reduce runtimes\n",
    "pointcloud_kwargs = {\"epsilon\": 0.25}\n",
    "sinkhorn_kwargs = {\"max_iterations\": 100}\n",
    "def resample_ot_jit(key, x_particles_prev, logw):\n",
    "    return resampler.resample_ot(key, x_particles_prev, logw,\n",
    "                                 pointcloud_kwargs = pointcloud_kwargs,\n",
    "                                 sinkhorn_kwargs = sinkhorn_kwargs)\n",
    "\n",
    "# OTT resampling\n",
    "lv_ott = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_lv,\n",
    "    model = lotvol_model,\n",
    "    key=subkey,\n",
    "    n_particles=n_particles_ot,\n",
    "    resample_fn = resample_ot_jit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lims_lv = jnp.log(jnp.array([\n",
    "    [0.9, 1.1],\n",
    "    [0.95, 1.05],\n",
    "    [3.9, 4.1],\n",
    "    [0.95, 1.05],\n",
    "    [0.075, 0.1],\n",
    "    [0.01, 0.2],\n",
    "    [0.15, 0.35],\n",
    "    [0.15, 0.35]]))\n",
    "\n",
    "plot_args_lv = {\n",
    "    \"theta_true\": theta_true_lv,\n",
    "    \"theta_lims\": theta_lims_lv,\n",
    "    \"theta_names\": theta_names_lv,\n",
    "    \"n_pts\": 50\n",
    "}\n",
    "\n",
    "# function to create projection plots for each log-likelihood estimator:\n",
    "ott_df = loglik_compare(\n",
    "    [lv_ott],\n",
    "    [\"Optimal Transport\"],\n",
    "    **plot_args_lv\n",
    ")\n",
    "\n",
    "grid = sns.relplot(\n",
    "    data=ott_df,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-loglikelihood\", \n",
    "    hue=\"Resampler\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 4,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None);\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta_true_lv[ind], color='red', lw = 0.9, label=\"True\")\n",
    "_ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e8585",
   "metadata": {},
   "source": [
    "### Check Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ec945",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotvol_neg_loglik = partial(neg_loglik, y_meas = y_meas_lv, model=lotvol_model)\n",
    "\n",
    "# various score functions\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def lv_score_multinom(theta, key, n_particles):\n",
    "    return jax.grad(lotvol_neg_loglik)(theta, key=key, n_particles=n_particles)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def lv_score_mvn(theta, key, n_particles):\n",
    "    return jax.grad(lotvol_neg_loglik)(theta, key=key, n_particles=n_particles,\n",
    "                                       resample_fn=resampler.resample_mvn)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def lv_score_gc(theta, key, n_particles):\n",
    "    return jax.grad(lotvol_neg_loglik)(theta, key=key, n_particles=n_particles,\n",
    "                                       resample_fn=resampler.resample_gaussian_copula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ff13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate log-likelihood with cubic regression: \n",
    "key, subkey = random.split(key)\n",
    "lv_multinom_true = jax.jit(partial(\n",
    "    neg_loglik,\n",
    "    y_meas = y_meas_lv,\n",
    "    model = lotvol_model,\n",
    "    key=subkey,\n",
    "    n_particles=10_00,\n",
    "    resample_fn = resampler.resample_multinomial))\n",
    "\n",
    "true_score, data = approx_score_fn(\n",
    "    lv_multinom_true, theta_true_lv, theta_lims_lv, theta_names_lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot true log-likelihood and its regression approximation\n",
    "plot_data = pd.melt(data, id_vars=[\"x\", \"x_opt\", \"loglik_grad\"], value_name=\"neg-ll\", var_name=\"method\")\n",
    "\n",
    "# rename for nicer plot: \n",
    "plot_data.loc[plot_data.method == \"y\", \"method\"] = \"True Log-lik\"\n",
    "plot_data.loc[plot_data.method == \"y_hat\", \"method\"] = \"Approx. Log-lik\"\n",
    "\n",
    "grid = sns.relplot(\n",
    "    data = plot_data,\n",
    "    kind=\"line\",\n",
    "    x=\"x\", y=\"neg-ll\", \n",
    "    hue = \"method\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 4,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None)\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta_true_lv[ind], color='red', lw = 0.9, label=\"True\")\n",
    "_ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "n_particles = 100\n",
    "n_sim = 30\n",
    "key, *subkeys = random.split(key, n_sim+1)\n",
    "\n",
    "# repeat calculation nsim times\n",
    "score_auto = []\n",
    "score_mvn = []\n",
    "score_gc = []\n",
    "\n",
    "for i in range(n_sim):\n",
    "    score_auto += [lv_score_multinom(theta_true_lv, subkeys[i], n_particles)]\n",
    "    score_mvn += [lv_score_mvn(theta_true_lv, subkeys[i], n_particles)]\n",
    "    score_gc += [lv_score_gc(theta_true_lv, subkeys[i], n_particles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (\n",
    "    pd.DataFrame({\n",
    "    \"theta\": np.tile(theta_names_lv, n_sim),\n",
    "    \"Multinomial\": np.array(score_auto).ravel(),\n",
    "    \"MVN\": np.array(score_mvn).ravel(),\n",
    "    \"Gaussian Copula\": np.array(score_gc).ravel()\n",
    "})\n",
    "    .melt(id_vars=[\"theta\"], value_vars=[\"Multinomial\", \"MVN\", \"Gaussian Copula\"], var_name=\"method\")\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_df, kind=\"box\",\n",
    "    x=\"method\", y=\"value\",\n",
    "    col=\"theta\",\n",
    "    col_wrap=4,\n",
    "    sharey=False\n",
    ")\n",
    "\n",
    "[g.axes[i].axhline(true_score[i]) for i in range(true_score.size)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74542bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cc80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedeb84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd773c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed08af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994acfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b58d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b5445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f2d1dc",
   "metadata": {},
   "source": [
    "## Work-Precision Plots\n",
    "\n",
    "- Benchmark: Take true params and 5k particles (lots of particles)\n",
    "- Run basic pf with a diff number of particles with diff seeds each time\n",
    "- Evaluate the log-lik on each of these particles 100 times each (multiple times each)\n",
    "- Record the runtime per random seed, for each of the number of particles\n",
    "- Calculate RMSE for each of the true params and estimated params for each # of particles\n",
    "- Plot (clock time VS accuracy) for each method. Doesn't have to be the same number of particle for each method, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ced61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9fccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_for (loglik, num_particles, true_params, init_params, n_iters, learning_rates, num_sim=10):\n",
    "    times = np.zeros((len(num_particles), num_sim))\n",
    "    mse = np.zeros((len(num_particles), num_sim))\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for i in range(len(num_particles)): \n",
    "        print(\"Iteration: \", i)\n",
    "        \n",
    "        for j in range(num_sim):\n",
    "            key, subkey = random.split(key)\n",
    "            start = time.perf_counter()\n",
    "            if j % 5 == 0:\n",
    "                print(\"   Inner iter: \", j)\n",
    "            _params, _losses, _steps, _time = simple_grad_descent2(\n",
    "                subkey = subkey,\n",
    "                model = lotvol_model,\n",
    "                grad_fun = loglik,\n",
    "                y_meas = y_meas,\n",
    "                theta = init_params,\n",
    "                learning_rate = learning_rates[i],\n",
    "                n_iter = n_iters[i],\n",
    "                n_particles = num_particles[i],\n",
    "                mask=jnp.ones(len(init_params))\n",
    "            )\n",
    "            times[i, j] = time.perf_counter() - start # work measured in clock time\n",
    "            mse[i, j] = sum((_params - true_params)**2) # MSE of estimates\n",
    "            \n",
    "    return times, mse, num_particles\n",
    "\n",
    "\n",
    "def run_sim (loglik, num_particles, true_params, init_params, n_iters, learning_rates, num_sim=10):\n",
    "    times = np.zeros(len(num_particles))\n",
    "    mse = np.zeros(len(num_particles))\n",
    "    key = random.PRNGKey(10)\n",
    "    results = []\n",
    "    for i in tqdm.tqdm(range(len(num_particles))): \n",
    "        key, *subkeys = random.split(key, num_sim+1)\n",
    "        start = time.perf_counter()\n",
    "        sim_res = jax.vmap(lambda key, lr, n_iter, n_part:\n",
    "             grad_descent_scan(\n",
    "                 key = key,\n",
    "                 model = lotvol_model,\n",
    "                 grad_fun = loglik,\n",
    "                 y_meas = y_meas,\n",
    "                 theta = init_params,\n",
    "                 learning_rate = lr,\n",
    "                 n_iter = n_iter,\n",
    "                 n_particles = n_part,\n",
    "                 mask=jnp.ones(len(init_params))),\n",
    "             in_axes = (0, None, None, None))(jnp.array(subkeys), learning_rates[i], n_iters[i], num_particles[i])\n",
    "        end = time.perf_counter()\n",
    "        mse[i] = (jnp.mean((sim_res[\"theta\"] - true_params)**2)) # MSE averaged over runs\n",
    "        times[i] = ((end - start)/num_sim) # average times\n",
    "        results.append(sim_res)\n",
    "    return times, mse, num_particles, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinom_particles = [50, 100, 250, 500, 1000]\n",
    "multinom_times, multinom_mse, multinom_num_particles, multinom_results = run_sim(\n",
    "    loglik = particle_neg_loglik_bootstrap,\n",
    "    num_particles = multinom_particles,\n",
    "    true_params = theta,\n",
    "    init_params = init_params,\n",
    "    n_iters = [500, 500, 500, 500, 500],\n",
    "    learning_rates = [1e-3, 1e-3, 1e-3, 1e-3, 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8514df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_particles = [10, 25, 50, 100]\n",
    "gc_times, gc_mse, gc_num_particles, gc_results = run_sim(\n",
    "    loglik = particle_neg_loglik_gc,\n",
    "    num_particles = gc_particles,\n",
    "    true_params = theta,\n",
    "    init_params = init_params,\n",
    "    n_iters = [500, 500, 500, 500, 500],\n",
    "    learning_rates = [1e-3, 1e-3, 1e-3, 1e-3, 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn_particles = [50, 100, 250, 500, 1000]\n",
    "mvn_times, mvn_mse, mvn_num_particles, mvn_results = run_sim(\n",
    "    loglik = particle_neg_loglik_mvn,\n",
    "    num_particles = mvn_particles,\n",
    "    true_params = theta,\n",
    "    init_params = init_params,\n",
    "    n_iters = [500, 500, 500, 500, 500],\n",
    "    learning_rates = [1e-3, 1e-3, 1e-3, 1e-3, 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 4), sharey = False)\n",
    "\n",
    "ax[0].plot(mvn_times, mvn_mse, label = \"MVN\")\n",
    "ax[0].plot(gc_times, gc_mse, label = \"GC\")\n",
    "ax[0].plot(multinom_times, multinom_mse, label = \"Multinomial\")\n",
    "ax[0].set(title=\"MSE VS Runtime\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(multinom_particles, mvn_times,  label = \"MVN Resampling\")\n",
    "ax[1].plot(gc_particles, gc_times,  label = \"GC Resampling\")\n",
    "ax[1].plot(mvn_particles, multinom_times,  label = \"Multinomial Resampling\")\n",
    "ax[1].set(title=\"Number of Particles VS Runtime\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(list(map(lambda x: np.min(x[\"losses\"]), mvn_results)), label = \"MVNN\")\n",
    "ax[2].plot(list(map(lambda x: np.min(x[\"losses\"]), gc_results)), label = \"GC\")\n",
    "ax[2].plot(list(map(lambda x: np.min(x[\"losses\"]), multinom_results)), label = \"Multinomial\");\n",
    "ax[2].set(title=\"Min losses after Gradient Descent\")\n",
    "ax[2].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPERIMENT: MSE VS number of particles for MVN: \n",
    "mvn_particles = np.arange(75, 500, step = 75)\n",
    "mvn_times, mvn_mse, mvn_num_particles, mvn_results = run_sim(\n",
    "    loglik = particle_neg_loglik_mvn,\n",
    "    num_particles = mvn_particles,\n",
    "    true_params = theta,\n",
    "    init_params = init_params,\n",
    "#     n_iters = [500] * len(mvn_particles),\n",
    "    n_iters = [500, 750, 1000, 1200, 1500, 2000],\n",
    "    learning_rates = [1e-3] * len(mvn_particles),\n",
    "    num_sim=100)\n",
    "\n",
    "multinom_particles = np.arange(75, 500, step = 75)\n",
    "multinom_times, multinom_mse, multinom_num_particles, multinom_results = run_sim(\n",
    "    loglik = particle_neg_loglik_bootstrap,\n",
    "    num_particles = multinom_particles,\n",
    "    true_params = theta,\n",
    "    init_params = init_params,\n",
    "#     n_iters = [500] * len(multinom_particles),\n",
    "    n_iters = [500, 750, 1000, 1200, 1500, 2000],\n",
    "    learning_rates = [1e-3] * len(multinom_particles),\n",
    "    num_sim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharey = True)\n",
    "\n",
    "ax[0].plot(mvn_particles, mvn_mse, label = \"MVN\")\n",
    "ax[1].plot(multinom_particles, multinom_mse, label = \"multinom\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ffcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625385e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed783b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f5478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e6de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae232543",
   "metadata": {},
   "source": [
    "### Findings and Questions\n",
    "\n",
    "- As the number of particles increases, the minimum loss from optimization also increases. Why would this happen? Shouldn't it be around the same value? \n",
    "- Not much of an improvement in the MSE when we increase the number of particles\n",
    "\n",
    "- **Q:** Should we use a constant seed for gradient descent? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f7a92",
   "metadata": {},
   "source": [
    "# Brownian Motion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1735ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfjax.models import BMModel\n",
    "\n",
    "# initial key for random numbers\n",
    "key = random.PRNGKey(0)\n",
    "sns.set_style(\"darkgrid\")\n",
    "# parameter values\n",
    "mu = 5.\n",
    "sigma = .2\n",
    "tau = 1.\n",
    "theta_true = jnp.array([mu, sigma, tau])\n",
    "bm_theta_names = [\"mu\", \"sigma\", \"tau\"]\n",
    "\n",
    "# data specification\n",
    "dt = .2\n",
    "n_obs = 100\n",
    "x_init = jnp.array(0.)\n",
    "\n",
    "# simulate data\n",
    "bm_model = BMModel(dt=dt)\n",
    "key, subkey = random.split(key)\n",
    "y_meas, x_state = pf.simulate(bm_model, subkey, n_obs, x_init, theta_true)\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"x_state\": jnp.squeeze(x_state),\n",
    "                         \"y_meas\": jnp.squeeze(y_meas)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\"\n",
    ")\n",
    "plt.title(\"Latent Variable(x_state) and Observation(y_meas)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_theta_lims = jnp.array([\n",
    "    [4.8, 5.2],\n",
    "    [0.1, .3],\n",
    "    [0.9, 1.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09faf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_neg_loglik_mvn(theta, key, n_particles=500):\n",
    "    \"\"\" negative log-likelihood of PF with MVN resampling \"\"\"\n",
    "    ret = pf.particle_filter(\n",
    "        theta=theta, \n",
    "        model=bm_model, \n",
    "        y_meas=y_meas, \n",
    "        n_particles=n_particles, \n",
    "        key=key,\n",
    "        resampler = resampler.resample_mvn)\n",
    "    return -ret[\"loglik\"]\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_neg_loglik_gc(theta, key, n_particles=250):\n",
    "    \"\"\" negative log-likelihood of PF with GC resampling \"\"\"\n",
    "    ret = pf.particle_filter(\n",
    "        theta=theta, \n",
    "        model=bm_model, \n",
    "        y_meas=y_meas, \n",
    "        n_particles=n_particles, \n",
    "        key=key,\n",
    "        resampler = resampler.resample_gaussian_copula)\n",
    "    return -ret[\"loglik\"]\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def bm_neg_loglik_bootstrap(theta, key, n_particles=500):\n",
    "    \"\"\" negative log-likelihood of PF with multinomial resampling \"\"\"\n",
    "    ret = pf.particle_filter(\n",
    "        theta=theta, \n",
    "        model=bm_model, \n",
    "        y_meas=y_meas, \n",
    "        n_particles=n_particles, \n",
    "        key=key)\n",
    "    return -ret[\"loglik\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ab502",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate projection plot for exact loglikelihood\n",
    "df_gc = pjp.proj_plot(\n",
    "    fun = lambda x: bm_neg_loglik_gc(x, key=_key, n_particles=50), \n",
    "    x_opt=theta_true, \n",
    "    x_lims=bm_theta_lims, \n",
    "    x_names=bm_theta_names, \n",
    "    n_pts=300,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "\n",
    "# calculate projection plot for exact loglikelihood\n",
    "df_mvn = pjp.proj_plot(\n",
    "    fun = lambda x: bm_neg_loglik_mvn(x, key=_key, n_particles=50), \n",
    "    x_opt=theta_true, \n",
    "    x_lims=bm_theta_lims, \n",
    "    x_names=bm_theta_names, \n",
    "    n_pts=300,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# calculate projection plot for particle filter loglikelihood\n",
    "df_multinom = pjp.proj_plot(\n",
    "    fun=lambda x: bm_neg_loglik_bootstrap(x, key=_key, n_particles=50),\n",
    "    x_opt=theta_true, \n",
    "    x_lims=bm_theta_lims, \n",
    "    x_names=bm_theta_names, \n",
    "    n_pts=300,\n",
    "    plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe687cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data frames and plot them\n",
    "plot_df = pd.concat([df_gc, df_multinom, df_mvn], ignore_index=True)\n",
    "plot_df[\"Resampler\"] = np.repeat([\"Gaussian Copula\", \"Multinomial\", \"MVN\"], len(df_gc[\"x_opt\"]))\n",
    "plot_df = plot_df.rename(columns={\"y\": \"neg-loglikelihood\"})\n",
    "\n",
    "grid = sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"x\", y=\"neg-loglikelihood\", \n",
    "    hue=\"Resampler\",\n",
    "    col=\"x_opt\",\n",
    "    col_wrap = 4,\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "grid.set(xlabel=None)\n",
    "\n",
    "for ind, _ax in enumerate(grid.axes.flat):\n",
    "    _ax.axvline(theta[ind], color='red', lw = 0.9, label=\"True\")\n",
    "    _ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247b040",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(<a id=\"cit-corenflos_etal21\" href=\"#call-corenflos_etal21\">Corenflos, Thornton <em>et al.</em>, 2021</a>) A. Corenflos, J. Thornton, G. Deligiannidis <em>et al.</em>, ``_Differentiable particle filtering via entropy-regularized optimal transport_'', Proceedings of the 38th international conference on machine learning, July 2021.  [online](https://proceedings.mlr.press/v139/corenflos21a.html)\n",
    "\n",
    "(<a id=\"cit-cappe_moulines05\" href=\"#call-cappe_moulines05\">Cappé and Moulines, 2005</a>) O. Cappé and E. Moulines, ``_On the use of particle filtering for maximum likelihood parameter estimation_'', 13th European Signal Processing Conference,  2005.\n",
    "\n",
    "(<a id=\"cit-poyiadjis_etal11\" href=\"#call-poyiadjis_etal11\">Poyiadjis, Doucet <em>et al.</em>, 2011</a>) Poyiadjis G., Doucet A. and Singh S. S., ``_Particle approximations of the score and observed information matrix in state space models with application to parameter estimation_'', Biometrika, vol. 98, number 1, pp. 65--80,  2011.  [online](https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asq062)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
