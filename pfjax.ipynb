{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFJAX: Particle Filtering with JAX\n",
    "\n",
    "**Martin Lysy, University of Waterloo**\n",
    "\n",
    "**November 11, 2021**\n",
    "\n",
    "## Summary\n",
    "\n",
    "The goal is to provide a fast implementation of a particle filter to estimate the marginal likelihood of a state-space model,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\theta) & = p(y_{0:T} \\mid \\theta) \\\\\n",
    "& = \\int \\prod_{t=0}^T p(y_t \\mid x_t, \\theta) \\times \\prod_{t=1}^T p(x_t \\mid x_{t-1}, \\theta) \\times p(x_0 \\mid \\theta)\\, d x_{0:T},   \n",
    "\\end{aligned}\n",
    "$$\n",
    "along with its gradient, $\\nabla \\mathcal{L}(\\theta) = \\frac{\\partial}{\\partial \\theta} \\mathcal{L}(\\theta)$.  The [**JAX**](https://github.com/google/jax) library is used to efficiently compose jit and autodiff operations  in order to achieve this.\n",
    "\n",
    "## API\n",
    "\n",
    "For given $\\theta$, the particle filter begins by running an importance sampler for $p(x_0 \\mid y_0, \\theta)$.  In order to do this, the user supplies a sampling proposal distribution,\n",
    "\n",
    "$$\n",
    "x_0 \\sim q(x_0 \\mid y_0, \\theta),\n",
    "$$\n",
    "\n",
    "along with a function to calculate the importance log weights,\n",
    "\n",
    "$$\n",
    "w(x_0 \\mid y_0, \\theta) = \\log p(y_0 \\mid x_0, \\theta) + \\log p(x_0 \\mid \\theta) - \\log q(x_0 \\mid y_0, \\theta).\n",
    "$$\n",
    "\n",
    "Since sampling from $q(x_0 \\mid y_0, \\theta)$ and computing $w(x_0 \\mid y_0, \\theta)$ involve many shared calculations, **pfjax** requires both to be performed in a single function:\n",
    "\n",
    "```python\n",
    "def pf_init(y_init, theta, key):\n",
    "    # calculate x_init and logw\n",
    "    return (x_init, logw)\n",
    "```\n",
    "\n",
    "Now suppose we have run the particle filter until step $t-1$.  In other words, the input is a weighted set of particles $((x_{t-1}^{(1)}, w_{t-1}^{(1)}), \\ldots, (x_{t-1}^{(B)}, w_{t-1}^{(B)}))$ representing the distribution $p(x_{t-1} \\mid y_{0:t-1}, \\theta)$.  In order to get to a weighted set of particles representing $p(x_t \\mid y_t, \\theta)$, the user suppliles a sampling proposal distribution\n",
    "\n",
    "$$\n",
    "x_t \\sim q(x_t \\mid x_{t-1}, y_t, \\theta)\n",
    "$$\n",
    "\n",
    "and a log-weight function\n",
    "\n",
    "$$\n",
    "w(x_t \\mid x_{t-1}, y_t, \\theta) = \\log p(y_t \\mid x_{t-1}, \\theta) + \\log p(x_t \\mid x_{t-1}, \\theta) - \\log q(x_t \\mid x_{t-1}, y_t, \\theta).\n",
    "$$\n",
    "\n",
    "Again, these are supplied as a single function:\n",
    "\n",
    "```python\n",
    "def pf_step(x_prev, y_curr, theta, key):\n",
    "    # claculate x_curr and logw\n",
    "    return (x_curr, logw)\n",
    "```\n",
    "\n",
    "## Notation\n",
    "\n",
    "### Functions\n",
    "\n",
    "- `state_lpdf(x_curr, x_last, theta)`: Log-density of $p(x_t | x_{t-1}, \\theta)$.\n",
    "- `state_sample(x_last, theta)`: Sample from $p(x_t | x_{t-1}, \\theta)$.\n",
    "- `meas_lpdf(y_curr, x_curr, theta)`: Log-density of $p(y_t | x_t, \\theta)$.\n",
    "- `meas_sample(x_curr, theta)`: Sample from $p(y_t | x_t, \\theta)$.\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- `n_obs`: Number of time points.\n",
    "- `n_particle`: Number of particles.\n",
    "- `n_state`: Tuple of state dimensions.\n",
    "- `n_meas`: Tuple of measured dimensions.\n",
    "\n",
    "## TODO\n",
    "\n",
    "- [x] Test against true BM likelihood.\n",
    "\n",
    "    Seem to get reasonable projection plots and a reasonable stochastic optimization.\n",
    "\n",
    "- [x] Data should be $y_{0:T}$ instead of $y_{1:T}$.  This has been changed in some of the documentation but not all of it...\n",
    "\n",
    "- [x] Add arbitrary prior specification $p(x_0 \\mid \\theta)$.  Should also be able to specify $x_0$ directly to calculate the marginal likelihood $\\mathcal{L}(x_0, \\theta) = p(y_{0:T} \\mid x_0, \\theta)$.\n",
    "\n",
    "- [ ] Example where `n_state` and `n_meas` are not scalar.  This can be SDE with `n_res > 1`.\n",
    "\n",
    "    **Update:** Implemented but not tested...\n",
    "\n",
    "- [x] Interface for supplying new models to generic PF code.  Currently we have the following:\n",
    "\n",
    "    ```python\n",
    "    import pfjax as pf # generic code\n",
    "    from my_model import MyModel # user-defined model\n",
    "\n",
    "    # pf estimate of marginal loglikelihood\n",
    "    def marginal_loglik(model, y_meas, theta, n_particles, key):\n",
    "        out = pf.particle_filter(model, y_meas, theta, n_particles, key)\n",
    "        return pf.particle_loglik(out[\"logw_particles\"])\n",
    "    \n",
    "    # construct model object\n",
    "    model = MyModel(dt=dt) # set dt as a data member\n",
    "    marginal_loglik(model, y_meas, theta, n_particles, key)\n",
    "    \n",
    "    # jit + grad version\n",
    "    marginal_loglik_jgrad = jax.jit(jax.grad(marginal_loglik, argnums=2), static_argnums=(0, 3))\n",
    "    marginal_loglik_jgrad(model, y_meas, theta, n_particles, key)\n",
    "    ```\n",
    "    \n",
    "    **WARNING:** The following code does not behave as \"expected\":\n",
    "    \n",
    "    ```python\n",
    "    model.dt = 2 * dt # update value of dt\n",
    "    marginal_loglik_jgrad(model, y_meas, theta, n_particles, key) # WARNING: uses the original dt...\n",
    "    ```\n",
    "    \n",
    "    In other words, JAX does not recompile `marginal_loglik_jgrad()` with the updated value of `model`, even though we've flagged it with `static_argnums`.\n",
    "    \n",
    "- [ ] Add arbitrary proposal distribution $q(x_t \\mid x_{t-1}, \\theta)$.  This should perhaps be done using:\n",
    "\n",
    "    ```python\n",
    "    step_sample(x_prev, t, theta)\n",
    "    step_logw(x_curr, x_prev, t, theta)\n",
    "    ```\n",
    "    \n",
    "    instead of e.g., `state_prop_lpdf()`, `state_targ_lpdf()` and `meas_lpdf()` which gets assembled internally for the user.  This is what was done in [**SMCTC**](https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/smctc/).  But what are the right args to these functions?  Sort of depends if they know about the \"global\" `y_meas`.  I think the right compromise is:\n",
    "    \n",
    "    ```python\n",
    "    step_sample(x_prev, y_curr, t, theta)\n",
    "    step_logw(x_curr, x_prev, y_curr, t, theta)\n",
    "    ```\n",
    "    \n",
    "    In other words, can use `y_curr` for an optimal filter, can support non-homogeneous state-space models via `t`, and anything more complicated will require globals.\n",
    "    \n",
    "- [x] Add proper unit tests.  But of what?  Seems like JAX and NumPy are so similar that checking code from one against the other will hardly be that helpful...\n",
    "\n",
    "    So, decided to test for-loop vs `vmap`/`xmap`/`lax.scan` etc always using JAX.  Reasoning is that without these constructs, JAX is identical to NumPy except PRNGs.\n",
    "    \n",
    "    Also testing OOP vs globals interface and jit + grad.\n",
    "\n",
    "- [ ] Add CPU parallelism support.\n",
    "\n",
    "    **Update:** Have started on an `xmap` implementation for this.  Trick will be to divide the `n_particles` axis into a 2D axis, in which the first is between (CPU) devices and the second is within each device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Brownian motion with drift\n",
    "\n",
    "The model is\n",
    "$$\n",
    "\\newcommand{\\N}{\\mathcal{N}}\n",
    "\\newcommand{\\dt}{\\Delta t}\n",
    "\\begin{aligned}\n",
    "x_0 & \\sim \\pi(x_0) \\\\\n",
    "x_t & \\sim \\N(x_{t-1} + \\mu \\dt, \\sigma^2 \\dt) \\\\\n",
    "y_t & \\sim \\N(x_t, \\tau^2).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, `n_state` = `n_meas` = 1, and we have a constant $\\dt$ through which we never need to take derivatives.  \n",
    "\n",
    "Note that with $\\pi(x_0) \\propto 1$, we may condition on $y_0$ and obtain $x_0 \\mid y_0 \\sim \\N(y_0, \\tau^2)$.\n",
    "\n",
    "### Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import jax.random as random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# module containing pfjax (for now)\n",
    "import particle_filter as pf\n",
    "# module containing implementation of BM model\n",
    "import bm_model as bm\n",
    "# utility for plotting\n",
    "from proj_data import proj_data\n",
    "\n",
    "# initial key for random numbers\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# parameter values\n",
    "mu = 5.\n",
    "sigma = .2\n",
    "tau = 1.\n",
    "theta = np.array([mu, sigma, tau])\n",
    "\n",
    "# data specification\n",
    "dt = .2\n",
    "n_obs = 100\n",
    "x_init = jnp.array([0.])\n",
    "\n",
    "# simulate data\n",
    "bm_model = bm.BMModel(dt=dt)\n",
    "key, subkey = random.split(key)\n",
    "y_meas, x_state = pf.meas_sim(bm_model, n_obs, x_init, theta, subkey)\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"x_state\": jnp.squeeze(x_state),\n",
    "                         \"y_meas\": jnp.squeeze(y_meas)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle filter: Speed test with jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particle filter specification\n",
    "n_particles = 100\n",
    "\n",
    "# timing without jit\n",
    "key, subkey = random.split(key)\n",
    "%timeit pf.particle_filter(bm_model, y_meas, theta, n_particles, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing with jit\n",
    "van_pf = jax.jit(pf.particle_filter, static_argnums=(0,3))(bm_model, y_meas, theta, n_particles, subkey)\n",
    "%timeit jax.jit(pf.particle_filter, static_argnums=(0,3))(bm_model, y_meas, theta, n_particles, subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test against exact likelihood\n",
    "\n",
    "The exact likelihood is defined as $p(y_{1:T} \\mid y_0, \\theta)$ rather than $p(y_{0:T} \\mid \\theta)$.  The reason is that the latter expression requires one to integrate over $x_0 \\sim \\pi(x_0)$, which can only be done when $\\pi(x_0)$ is a proper prior.  However, for our choice of $\\pi(x_0) \\propto 1$ this is not the case.  On the other hand, $p(y_{1:T} \\mid y_0, \\theta)$ only requires us to integrate over $p(x_0 \\mid y_0, \\theta)$, which only requires the posterior to be proper (which is always the case for valid Bayesian inference).\n",
    "\n",
    "Conditioned on $x_0$ and $\\theta$, the Brownian latent variables $x_{1:T}$ are multivariate normal with\n",
    "$$\n",
    "\\newcommand{\\cov}{\\operatorname{cov}}\n",
    "\\begin{aligned}\n",
    "%E[x_0 \\mid \\theta] & = y_0 & \\var(x_0\\\\\n",
    "E[x_t \\mid x_0, \\theta] & = x_0 + \\mu t, \\\\\n",
    "\\cov(x_s, x_t \\mid x_0, \\theta) & = \\sigma^2 \\min(s, t).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Conditioned on $x_{0:T}$ and $\\theta$, the measurement variables $y_{1:T}$ are multivariate normal with\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid x_{0:T}, \\theta] & = x_{1:T}, \\\\\n",
    "\\cov(y_s, y_t \\mid x_{0:T}, \\theta) & = \\tau^2 \\delta_{st}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Therefore, the marginal distribution of $y_{1:T}$ is multivariate normal with\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid x_0, \\theta] & = x_0 + \\mu t, \\\\\n",
    "\\cov(y_s, y_t \\mid x_0, \\theta) & = \\sigma^2 \\min(s, t) + \\tau^2 \\delta_{st}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "For the given choice of prior, we have $x_0 \\mid y_0 \\sim \\N(y_0, \\tau^2)$ for the initial observation $y_0$.  Integrating over $x_0$, the marginal distribution of $y_{1:T}$ is MVN with\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y_t \\mid y_0, \\theta] & = y_0 + \\mu t, \\\\\n",
    "\\cov(y_s, y_t \\mid y_0, \\theta) & = \\sigma^2 \\min(s, t) + \\tau^2(\\delta_{st} + 1).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def bm_loglik(y_meas, dt, theta):\n",
    "    \"\"\"\n",
    "    Exact loglikelihood of the BM model.\n",
    "    \"\"\"\n",
    "    mu = theta[0]\n",
    "    sigma2 = theta[1] * theta[1]\n",
    "    tau2 = theta[2] * theta[2]\n",
    "    n_obs = y_meas.shape[0]-1  # conditioning on y_0\n",
    "    t_meas = jnp.arange(1, n_obs+1) * dt\n",
    "    Sigma_y = sigma2 * jax.vmap(lambda t: jnp.minimum(t, t_meas))(t_meas) + \\\n",
    "        tau2 * (jnp.ones((n_obs, n_obs)) + jnp.eye(n_obs))\n",
    "    mu_y = y_meas[0] + mu * t_meas\n",
    "    return jsp.stats.multivariate_normal.logpdf(jnp.squeeze(y_meas[1:]), mu_y, Sigma_y)\n",
    "\n",
    "\n",
    "bm_loglik(y_meas, dt, np.array([5, 1.2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot exact likelihood\n",
    "theta_lims = np.array([[3., 8.], [0., .1], [.5, 2]])\n",
    "#theta_lims = np.array([[3., 8.], [.194, .195], [0, 10]])\n",
    "theta_names = [\"mu\", \"sigma\", \"tau\"]\n",
    "n_pts = 100\n",
    "\n",
    "# calculate projection plot\n",
    "plot_df = proj_data(lambda th: bm_loglik(y_meas, dt, th),\n",
    "                    theta, theta_lims, theta_names)\n",
    "\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"x\", y=\"y\", col=\"theta\",\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVN Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import particle_filter_mvn as pfmvn\n",
    "\n",
    "# timing without jit\n",
    "key, subkey = random.split(key)\n",
    "%timeit pfmvn.particle_filter(bm_model, y_meas, theta, n_particles, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing with jit\n",
    "pf_mvn = jax.jit(pfmvn.particle_filter, static_argnums=(0,3))(bm_model, y_meas, theta, n_particles, subkey)\n",
    "%timeit jax.jit(pfmvn.particle_filter, static_argnums=(0,3))(bm_model, y_meas, theta, n_particles, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_pf[\"logw_particles\"]\n",
    "\n",
    "pf_mvn[\"logw_particles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic loglikelihood using particle filter\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch\n",
    "\n",
    "Everything below is obsolete code that should not be expected to work as API evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output script file with Jupyter highlighting\n",
    "\n",
    "Just can't figure out how to resize output as desired..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output .py file with jupyter highlighting\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import HtmlFormatter\n",
    "import IPython\n",
    "\n",
    "with open('particle_filter.py') as f:\n",
    "    code = f.read()\n",
    "\n",
    "formatter = HtmlFormatter()\n",
    "IPython.display.HTML('<style type=\"text/css\">{}</style>{}'.format(\n",
    "    formatter.get_style_defs('.highlight'),\n",
    "    highlight(code, PythonLexer(), formatter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with `pmap`\n",
    "\n",
    "A few interesting threads:\n",
    "\n",
    "- [Here's](https://github.com/google/jax/issues/5506) to tell XLA to use multiple CPUs.\n",
    "- Not sure what happens if you set more devices than there are cores, so perhaps best to use `vmap` inside `pmap`.  However, could potentially do with [`xmap`](https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html) instead.  Note there an alternative way of setting the number of CPU devices from within Python, but may not actually dispatch to multiple cores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, p, v):\n",
    "        self.p = p\n",
    "        self.v = v\n",
    "        \n",
    "    def set_v(self, v):\n",
    "        self.v = v\n",
    "\n",
    "register_pytree_node(World,\n",
    "                    lambda x: ((x.p, x.v), None),\n",
    "                    lambda _, tup: World(tup[0], tup[1]))\n",
    "\n",
    "\n",
    "fun = jax.jit(lambda world, x: world.p + world.v + x, static_argnums=0)\n",
    "\n",
    "world = World(jnp.array([1.]), jnp.array([2.]))\n",
    "x = jnp.array([3.])\n",
    "\n",
    "y1 = fun(world, x)\n",
    "world.p = jnp.array([4.])\n",
    "y2 = fun(world, x)\n",
    "world.set_v(jnp.array([4.]))\n",
    "y3 = fun(world, x)\n",
    "world = World(jnp.array([1.]), jnp.array([4.]))\n",
    "y4 = fun(world, x)\n",
    "\n",
    "print(\"y1 = {}, y_2 = {}, y_3 = {}, y_4 = {}\".format(y1, y2, y3, y4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun2 = jax.jit(lambda world, x: world[\"p\"] + world[\"v\"] + x, static_argnums=0)\n",
    "\n",
    "world = {\"p\": jnp.array([1.]), \"v\": jnp.array([2.])}\n",
    "x = jnp.array(3.)\n",
    "\n",
    "y1 = fun2(world, x)\n",
    "world[\"v\"] = jnp.array([4.])\n",
    "y2 = fun2(world, x)\n",
    "world = {\"p\": jnp.array([1.]), \"v\": jnp.array([4.])}\n",
    "y3 = fun2(world, x)\n",
    "\n",
    "print(\"y1 = {}, y_2 = {}, y_3 = {}\".format(y1, y2, y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from proj_data import proj_data\n",
    "\n",
    "# Simulate some data\n",
    "key = random.PRNGKey(10)\n",
    "\n",
    "# parameter values\n",
    "mu = 5.\n",
    "sigma = .2\n",
    "tau = 1.\n",
    "theta = np.array([mu, sigma, tau])\n",
    "# data specification\n",
    "dt = .2\n",
    "n_obs = 100\n",
    "x_init = jnp.array([0.])\n",
    "\n",
    "# simulate data\n",
    "key, subkey = random.split(key)\n",
    "y_meas, x_state = meas_sim(n_obs, x_init, theta, subkey)\n",
    "\n",
    "# plot data\n",
    "plot_df = (pd.DataFrame({\"time\": jnp.arange(n_obs) * dt,\n",
    "                         \"state\": jnp.squeeze(x_state),\n",
    "                         \"meas\": jnp.squeeze(y_meas)})\n",
    "           .melt(id_vars=\"time\", var_name=\"type\"))\n",
    "sns.relplot(\n",
    "    data=plot_df, kind=\"line\",\n",
    "    x=\"time\", y=\"value\", hue=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with particle filter\n",
    "n_particles = 100\n",
    "theta = jnp.array([mu, sigma, tau])\n",
    "%timeit pf_out = particle_filter_scan(y_meas, theta, n_particles, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tau = 0, MLE of mu and sigma are analytically available\n",
    "mu_hat = jnp.mean(jnp.diff(jnp.squeeze(y_meas)))/dt\n",
    "sigma_hat = jnp.sqrt(jnp.mean((jnp.diff(jnp.squeeze(y_meas)) - mu*dt)**2)/dt)\n",
    "sigma_hat2 = jnp.std(jnp.diff(jnp.squeeze(y_meas))) / np.sqrt(dt)\n",
    "print(\"mu_hat = \", mu_hat)\n",
    "print(\"sigma_hat = \", sigma_hat)\n",
    "print(\"sigma_hat2 = \", sigma_hat2)\n",
    "print(\"theta = \", theta)\n",
    "# bm_loglik(y_meas, dt, np.array([mu, sigma, tau]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (pd.DataFrame(np.concatenate((plot_data[0], plot_data[1])).T,\n",
    "                        columns=[a+b for b in map(str, [\"_x\", \"_y\"]) for a in theta_names])\n",
    "           .melt()\n",
    "           )\n",
    "plot_df[[\"theta\", \"axis\"]] = plot_df[\"variable\"].str.split(\"_\", 1, expand=True)\n",
    "plot_df = plot_df[[\"axis\", \"theta\", \"value\"]]\n",
    "# plot_df\n",
    "plot_df.pivot(columns=\"axis\", values=[\"theta\", \"value\"])\n",
    "# df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n",
    "#                           'two'],\n",
    "#                   'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "#                   'baz': [1, 2, 3, 4, 5, 6],\n",
    "#                   'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n",
    "# df\n",
    "#df.pivot(index='foo', columns='bar', values='baz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    jnp.array([[4, 7, 10],\n",
    "               [5, 8, 11],\n",
    "               [6, 9, 12]]),\n",
    "    #index=[1, 2, 3],\n",
    "    columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how smooth random.choice is\n",
    "\n",
    "key = random.PRNGKey(2)\n",
    "\n",
    "\n",
    "def fun(p):\n",
    "    \"\"\"\n",
    "    Sample one draw from a Bernoulli with probability `p`.\n",
    "    \"\"\"\n",
    "    return random.choice(key, a=jnp.arange(2), p=jnp.array([p, 1-p]))\n",
    "\n",
    "\n",
    "# see how output varies as we keep the seed fixed but let p range between 0 and 1\n",
    "n = 100\n",
    "prob = jnp.linspace(0, 1, n)\n",
    "x = jnp.zeros(n)\n",
    "for i in range(n):\n",
    "    x = x.at[i].set(fun(prob[i]))\n",
    "\n",
    "# result: sampler is indeed smooth in p.\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
