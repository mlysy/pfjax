{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d13772f",
   "metadata": {},
   "source": [
    "# Lotka-Volterra Model with Multiplicative Noise\n",
    "\n",
    "## Model Definition\n",
    "\n",
    "The SDE is given by $\\XX_t = (U_t, V_t)$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\ud U_t \\\\\n",
    "\\ud V_t\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix} \n",
    "\\alpha U_t - \\beta U_t V_t \\\\ \n",
    "\\beta U_t V_t - \\gamma V_t \n",
    "\\end{bmatrix} \\ud t + \n",
    "\\begin{bmatrix}\n",
    "\\alpha U_t + \\beta U_t V_t & - \\beta U_t V_t \\\\\n",
    "-\\beta U_t V_t & \\gamma V_t + \\beta U_t V_t\n",
    "\\end{bmatrix}^{1/2} \\ud \\BB_t.\n",
    "$$\n",
    "\n",
    "However, the SDE is actually approximated on the log scale, $\\ZZ_t = \\log \\XX_t$.  The noisy observations are given by \n",
    "\n",
    "$$\n",
    "\\yy_n \\ind \\N(\\exp(\\ZZ_n), \\diag(\\tta^2)).\n",
    "$$\n",
    "\n",
    "## Inference Settings\n",
    "\n",
    "- In each case, please try to estimate $\\pph = (\\alpha, \\beta, \\gamma)$, assuming that $\\tta$ is known.  \n",
    "\n",
    "- Please do this with stochastic optimization, and make sure you provide a hessian estimate for the variance at the end.  \n",
    "\n",
    "- You should use the \"Bayesian normal approximation\", which plots normal distributions with mean MLE and variance taken from the diagonal of the inverse Hessian.  Then put a vertical line for the true parameter value, as in the Ryder paper Figure 3.\n",
    "\n",
    "- Make sure you do both the optimization/hessian/etc on $\\log \\pph$.  You are welcome to rewrite the `LVMultModel` below with $\\tth = (\\log \\pph, \\log \\tta)$ as opposed to $\\tth = (\\pph, \\tta)$.\n",
    "\n",
    "### Setting 1\n",
    "\n",
    "The following settings are modified from Ryder et al (2018), Section 5.1:\n",
    "\n",
    "\n",
    "- $\\pph = (\\alpha, \\beta, \\gamma) = (0.5, 0.0025, 0.3)$.\n",
    "\n",
    "- $\\tta = (1, 1)$.\n",
    "\n",
    "- $\\XX_0 = (71, 79)$.\n",
    "\n",
    "- `dt = 1`.\n",
    "\n",
    "- `n_res = 10`.\n",
    "\n",
    "- `n_obs = 50`.\n",
    "\n",
    "### Setting 2\n",
    "\n",
    "**Update:** Ignore this for now.\n",
    "\n",
    "- Same as above, but $V_t$ is unobserved.  The easiest way to do this is to do the optimization/hessian/etc over $\\pph$ while fixing $\\tau = (1, 100)$.  So, the first $\\tau$ is its true value, but the second $\\tau$ is made so large that the corresponding observations provide no information about $\\pph$.\n",
    "\n",
    "- So, you simulate data with $\\pph = \\pph_{\\text{true}}$ and $\\tta = \\tta_{\\text{true}}$.  But for inference, you only optimize/hessian over $\\pph$, as in setting 1, but now you change the fixed parameters to $\\tta = (1, 100)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a855845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "from jax import random\n",
    "from jax import lax\n",
    "import pfjax as pf\n",
    "from pfjax import sde as sde\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "\n",
    "class LVMultModel(sde.SDEModel):\n",
    "    def __init__(self, dt, n_res, bootstrap=False):\n",
    "        self._observed_predator = observed_predator\n",
    "        self._dt = dt\n",
    "        self._n_res = n_res\n",
    "        self._n_state = (self._n_res, 2)\n",
    "        self._bootstrap = bootstrap\n",
    "        super().__init__(dt, n_res, diff_diag=False)\n",
    "\n",
    "    def _get_params(self, theta):\n",
    "        return theta[0], theta[1], theta[2], theta[3:4]\n",
    "\n",
    "    def _get_data(self, x):\n",
    "        return x[0], x[1]\n",
    "\n",
    "    def _drift(self, x, theta):\n",
    "        \"\"\"Drift on the original scale.\"\"\"\n",
    "        alpha, beta, gamma, tau = self._get_params(theta)\n",
    "        U, V = self._get_data(x)\n",
    "        aU = alpha * U\n",
    "        bUV = beta * U * V\n",
    "        gV = gamma * V\n",
    "        return jnp.array([aU - bUV, bUV - gV])\n",
    "\n",
    "    def _diff(self, x, theta):\n",
    "        \"\"\"Drift on the original scale.\"\"\"\n",
    "        alpha, beta, gamma, tau = self._get_params(theta)\n",
    "        U, V = self._get_data(x)\n",
    "        aU = alpha * U\n",
    "        bUV = beta * U * V\n",
    "        gV = gamma * V\n",
    "        return jnp.array([[aU + bUV, -bUV], [-bUV, gV + bUV]])\n",
    "\n",
    "    def _ito_inv(self, x):\n",
    "        \"\"\"\n",
    "        Inverse Ito transformation.\n",
    "\n",
    "        In this case the transformation is `log(x)`, so the inverse transformation is `exp(x)`.\n",
    "        \"\"\"\n",
    "        return jnp.exp(x)\n",
    "\n",
    "    def _ito_dx(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of Ito transformation.\n",
    "\n",
    "        If the transformation is `log(x)`, the derivative is `1/x`.\n",
    "        \"\"\"\n",
    "        return 1.0/x\n",
    "\n",
    "    def _ito_dx2(self, x):\n",
    "        \"\"\"\n",
    "        Second derivative of Ito transformation.\n",
    "\n",
    "        If the transformation is `log(x)`, the second derivative is `-1/x^2`.\n",
    "        \"\"\"\n",
    "        return -1.0 / (x * x)\n",
    "\n",
    "    def drift(self, x, theta):\n",
    "        \"\"\"\n",
    "        Drift upon applying Ito transformation.\n",
    "        \"\"\"\n",
    "        z = self._ito_inv(x)  # transform to original scale\n",
    "        dx = self._ito_dx(z)\n",
    "        dx2 = self._ito_dx2(z)\n",
    "        return dx * self._drift(z, theta) + \\\n",
    "            0.5 * dx2 * jnp.diag(self._diff(z, theta))\n",
    "\n",
    "    def diff(self, x, theta):\n",
    "        \"\"\"\n",
    "        Diffusion upon applying Ito transformation.\n",
    "        \"\"\"\n",
    "        z = self._ito_inv(x)  # transform to original scale\n",
    "        dx = self._ito_dx(z)\n",
    "        return jnp.outer(dx, dx) * self._diff(z, theta)\n",
    "\n",
    "    def meas_lpdf(self, y_curr, x_curr, theta):\n",
    "        r\"\"\"\n",
    "        Log-density of `p(y_curr | x_curr, theta)`.\n",
    "\n",
    "        FIXME: Explain the choice of distribution here.\n",
    "\n",
    "        Args:\n",
    "            y_curr: Measurement variable at current time `t`.\n",
    "            x_curr: State variable at current time `t`.\n",
    "            theta: Parameter value.\n",
    "\n",
    "        Returns\n",
    "            The log-density of `p(y_curr | x_curr, theta)`.\n",
    "        \"\"\"\n",
    "        (alpha, beta, gamma, tau) = self._get_params(theta)\n",
    "        return jnp.sum(\n",
    "            jsp.stats.norm.logpdf(y_curr,\n",
    "                                  loc=jnp.exp(x_curr[-1]), scale=tau)\n",
    "        )\n",
    "\n",
    "    def meas_sample(self, key, x_curr, theta):\n",
    "        r\"\"\"\n",
    "        Sample from `p(y_curr | x_curr, theta)`.\n",
    "\n",
    "        Args:\n",
    "            key: PRNG key.\n",
    "            x_curr: State variable at current time `t`.\n",
    "            theta: Parameter value.\n",
    "\n",
    "        Returns:\n",
    "            Sample of the measurement variable at current time `t`: `y_curr ~ p(y_curr | x_curr, theta)`.\n",
    "        \"\"\"\n",
    "        (alpha, beta, gamma, tau) = self._get_params(theta)\n",
    "        return jnp.exp(x_curr[-1]) + \\\n",
    "            tau * random.normal(key, (self._n_state[1],))\n",
    "\n",
    "    def pf_init(self, key, y_init, theta):\n",
    "        r\"\"\"\n",
    "        Importance sampler for `x_init`.  \n",
    "\n",
    "        See file comments for exact sampling distribution of `p(x_init | y_init, theta)`, i.e., we have a \"perfect\" importance sampler with `logw = CONST(theta)`.\n",
    "\n",
    "        Args:\n",
    "            key: PRNG key.\n",
    "            y_init: Measurement variable at initial time `t = 0`.\n",
    "            theta: Parameter value.\n",
    "\n",
    "        Returns:\n",
    "            - x_init: A sample from the proposal distribution for `x_init`.\n",
    "            - logw: The log-weight of `x_init`.\n",
    "        \"\"\"\n",
    "        (alpha, beta, gamma, tau) = self._get_params(theta)\n",
    "        key, subkey = random.split(key)\n",
    "        x_init = jnp.log(y_init + tau * random.truncated_normal(\n",
    "            subkey,\n",
    "            lower=-y_init/tau,\n",
    "            upper=jnp.inf,\n",
    "            shape=(self._n_state[1],)\n",
    "        ))\n",
    "        logw = jnp.sum(jsp.stats.norm.logcdf(y_init/tau))\n",
    "        return \\\n",
    "            jnp.append(jnp.zeros((self._n_res-1,) + x_init.shape),\n",
    "                       jnp.expand_dims(x_init, axis=0), axis=0), \\\n",
    "            logw\n",
    "\n",
    "    def pf_step(self, key, x_prev, y_curr, theta):\n",
    "        \"\"\"\n",
    "        Choose between bootstrap filter and bridge proposal.\n",
    "\n",
    "        Args:\n",
    "            x_prev: State variable at previous time `t-1`.\n",
    "            y_curr: Measurement variable at current time `t`.\n",
    "            theta: Parameter value.\n",
    "            key: PRNG key.\n",
    "\n",
    "        Returns:\n",
    "            - x_curr: Sample of the state variable at current time `t`: `x_curr ~ q(x_curr)`.\n",
    "            - logw: The log-weight of `x_curr`.\n",
    "        \"\"\"\n",
    "        if self._bootstrap:\n",
    "            x_curr, logw = super().pf_step(key, x_prev, y_curr, theta)\n",
    "        else:\n",
    "            (alpha, beta, gamma, tau) = self.get_params(theta)\n",
    "            omega = (tau / y_curr)**2\n",
    "            x_curr, logw = self.bridge_prop(\n",
    "                key=key,\n",
    "                x_prev=x_prev,\n",
    "                y_curr=y_curr,\n",
    "                theta=theta,\n",
    "                Y=jnp.log(y_curr),\n",
    "                A=jnp.eye(self._n_state[1]),\n",
    "                Omega=jnp.diag(omega)\n",
    "            )\n",
    "        return x_curr, logw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2acd2210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 70.21262487,  79.92433539],\n",
       "             [ 80.42041459,  80.25574707],\n",
       "             [111.96799467,  67.56168716],\n",
       "             [150.11845576,  66.22223218],\n",
       "             [198.75781923,  74.74067897],\n",
       "             [276.58517712,  93.53309362],\n",
       "             [366.87895763, 141.75974266],\n",
       "             [371.87596578, 293.92770769],\n",
       "             [222.32580286, 480.23536364],\n",
       "             [108.23886615, 534.23730638],\n",
       "             [ 33.82819555, 472.51697638],\n",
       "             [ 21.50549159, 381.08688888],\n",
       "             [ 20.72492078, 284.9783271 ],\n",
       "             [ 25.77922956, 208.36774137],\n",
       "             [ 28.82206524, 163.42226061],\n",
       "             [ 36.90370283, 117.95917004],\n",
       "             [ 55.96794108,  89.51758997],\n",
       "             [ 60.5158919 ,  76.82175295],\n",
       "             [ 90.2713489 ,  66.24808305],\n",
       "             [131.16006656,  59.91892593],\n",
       "             [192.18155443,  45.27773199],\n",
       "             [268.66083948,  48.91028414],\n",
       "             [379.43974795,  75.73259249],\n",
       "             [437.6632364 , 174.28386829],\n",
       "             [378.13994662, 364.447433  ],\n",
       "             [171.94546584, 572.73137475],\n",
       "             [ 68.30983505, 562.42024061],\n",
       "             [ 38.11301309, 466.04718286],\n",
       "             [ 22.26271754, 365.79196949],\n",
       "             [ 10.88314463, 276.7218031 ],\n",
       "             [ 10.39461072, 213.16805337],\n",
       "             [ 12.3716386 , 157.16954017],\n",
       "             [ 13.21705296, 125.47299628],\n",
       "             [ 18.46747658,  97.18245498],\n",
       "             [ 30.32144983,  77.61653933],\n",
       "             [ 28.21404037,  62.86679739],\n",
       "             [ 41.80176509,  49.58570111],\n",
       "             [ 60.116559  ,  39.03981077],\n",
       "             [ 98.74935567,  28.77953115],\n",
       "             [164.82904105,  27.60986047],\n",
       "             [253.99541904,  40.87631934],\n",
       "             [379.65828523,  79.86892169],\n",
       "             [489.6497204 , 155.42610968],\n",
       "             [447.42493205, 398.23119024],\n",
       "             [155.30121159, 719.89827629],\n",
       "             [ 48.59250496, 640.96400196],\n",
       "             [ 23.84214792, 517.78946061],\n",
       "             [ 21.81604681, 377.31470256],\n",
       "             [ 14.1487666 , 296.66541039],\n",
       "             [ 10.37865794, 228.93699945]], dtype=float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data from setting 1\n",
    "\n",
    "theta = jnp.array([.5, .0025, .3, 1., 1.])\n",
    "x0 = jnp.log(jnp.array([71., 79.]))\n",
    "\n",
    "dt = 1.\n",
    "n_res = 10\n",
    "n_obs = 50\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "x_init = jnp.block([[jnp.zeros((n_res-1, 2))], [x0]])\n",
    "lv_model = LVMultModel(dt=dt, n_res=n_res)\n",
    "(y_meas, x_state) = pf.simulate(\n",
    "    model=lv_model,\n",
    "    key=subkey,\n",
    "    n_obs=n_obs,\n",
    "    x_init=x_init,\n",
    "    theta=theta\n",
    ")\n",
    "y_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2f8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
